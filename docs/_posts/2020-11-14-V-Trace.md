---
title: "V-trace"
excerpt: "In which we theoretically analyze the V-trace target."
categories:
  - Reinforcement Learning
tags:
  - Reinforcement Learning
  - Mathematics
---

## Introduction

The V-trace loss, introduced by [Espeholt et al. 2018](#ref1), targets at near-on-policy data and has been successfully applied to solving challenging tasks such as [StarCraft II]({{ site.baseurl }}{% post_url 2021-01-14-AlphaStar %}). In this post, we theoretically analyze V-trace  

## V-trace

The V-trace target is defined as

$$
\begin{align}
v(x_t) &:= V(x_t)+\sum_{k=t}^{t+n-1}\gamma^{k-t}\left(\prod_{i=t}^{k-1}c_i\right)\delta_kV\tag{1}\label{eq:1}\\\
where\quad \delta_kV&:=\rho_k(r_k+\gamma V(x_{k+1})-V(x_k))\\\
c_{i}&:=\lambda \min\left(\bar c, {\pi(a_i|x_i)\over \mu(a_i|x_i)}\right)\\\
\rho_k&:=\min\left(\bar\rho, {\pi(a_k|x_k)\over \mu(a_k|x_k)}\right)
\end{align}
$$

The advantage of V-trace is that 1) it's a multi-step algorithm, allowing reward signal propagate through multiple steps and enables efficient learning; 2) it reduces the potentially infinite variance of traditional multi-step target with importance sampling by clipping the importance ratio at $$\bar c$$ and $$\bar\rho$$. In the next section, we analysis V-trace and conclude that it is only suitable for near-on-policy learning as the clipped importance sampling introduces some bias to the value function.

## Analysis

### V-trace Convergence

Denote Equation $$\eqref{eq:1}$$ as the V-trace operator $$\mathcal R$$: 

$$
\begin{align}
\mathcal RV(x_t) :=& V(x_t)+\mathbb E_\mu\left[\sum_{k\ge t}\gamma^{k-t}\left(\prod_{i=t}^{k-1}c_i\right)\delta_kV\right]\tag 2\\\
where\quad \delta_kV=&\rho_k(r_k+\gamma V(x_{k+1})-V(x_k))
\end{align}
$$

where the expectation $$\mathbb E_\mu$$ is with respect to the behavior policy $$\mu$$. Here we consider the infinite horizon operator but very similar results hold for the n-step truncated operator.

**Theorem 1.** *Let $$c_{i}=\lambda \min\left(\bar c, {\pi(a_i\vert x_i)\over \mu(a_i\vert x_i)}\right)$$ and $$\rho_k:=\min\left(\bar\rho, {\pi(a_k\vert x_k)\over \mu(x_k\vert x_k)}\right)$$ be the truncated importance sampling weights, with $$\bar\rho\ge \bar c$$. Assume that there exists $$\beta\in(0,1]$$ such that $$\mathbb E_\mu\rho_t\ge\beta$$. Then the operator $$\mathcal R$$ has a unique fixed point $$V^{\pi_{\bar\rho}}$$, which is the value function of the policy $$\pi_{\bar\rho}$$ defined by*

$$
\pi_{\bar\rho}(a|x)={\min(\bar\rho\mu(a|x),\pi(a|x))\over{\sum_{b\in A}\min(\bar\rho\mu(b|x),\pi(b|x))}}\tag 3
$$

*Furthermore, $$\mathcal R$$ is an $$\eta$$-contraction mapping in sup-norm with*

$$
\eta:=\gamma^{-1}-(\gamma^{-1}-1)\mathbb E_\mu\left[\sum_{k\ge t}\gamma^{k-t}\left(\prod_{i=t}^{k-2}c_i\right)\rho_{k-1}\right]\le 1-(1-\gamma)\beta< 1\tag 4
$$

Before we prove it, let's see the role of $$\bar c$$ and $$\bar \rho$$ play. $$\bar c$$ appearing the contraction modulus $$\eta$$ affects the speed at which V-trace converges to $$V^{\pi_\bar\rho}$$ -- a small $$\bar c$$ corresponds to lower variance but worse contraction rate. On the other hand, $$\bar\rho$$ influences the policy $$\pi_{\bar\rho}$$ and thus controls the fixed point $$V^{\pi_\bar\rho}$$. Moreover, $$\bar\rho$$ biases the policy and thus the value function whenever $$\bar\rho\mu(a\vert x)<\pi(a\vert x)$$, which makes the V-trace operator inappropriate for data way off policy. 

**Proof.** First notice that we can rewrite Equation $$\eqref{eq:2}$$ as

$$
\mathcal R V(x_t)=(1-\mathbb E_\mu\rho_t)V(x_t)+\mathbb E_\mu\left[\sum_{k\ge t}\gamma^{k-t}\left(\prod_{i=t}^{k-1}c_i\right)\big(\rho_kr_k+\gamma(\rho_k-c_k\rho_{k+1})V(x_{k+1})\big)\right]
$$

where we move $$-\rho_{k+1}V(x_{k+1})$$ in $$\delta_{k+1}V$$ into $$\delta_{k}V$$. Thus

$$
\begin{align}
\mathcal RV_1(x_{t})-\mathcal RV_2(x_t)&=(1-\mathbb E_\mu\rho_t)(V_1(x_t)-V_2(x_t))+\mathbb E_\mu\left[\sum_{k\ge t}\gamma^{k-t+1}\left(\prod_{i=t}^{k-1}c_i\right)\big((\rho_k-c_k\rho_{k+1})(V_1(x_{k+1})-V_2(x_{k+1})\big)\right]\\\
&=\mathbb E_\mu\left[\sum_{k\ge t}\gamma^{k-t}\left(\prod_{i=t}^{k-2}c_i\right)\big(\underbrace{(\rho_{k-1}-c_{k-1}\rho_{k})}_{a_k}(V_1(x_{k})-V_2(x_{k})\big)\right]
\end{align}
$$

with the notation that $$c_{t-1}=\rho_{t-1}=1$$ and $$\prod_{i=t}^{k-2}c_i=1$$ for $$k=t$$ and $$t+1$$. 

We shall see the coefficients $$(\alpha_k)_{k\ge t}$$ are non-negative in expectation. Because $$\bar\rho\ge \bar c$$, we have

$$
\mathbb E_\mu\alpha_k=\mathbb E_\mu[\rho_{k-1}-c_{k-1}\rho_k]\ge\mathbb E_\mu[c_{k-1}(1-\rho_k)]\ge 0
$$

Since $$\mathbb E_\mu\rho_k\le\mathbb E_\mu\log{\pi(a_k\vert x_k)\over\mu(a_k\vert x_k)}=1$$. Thus $$V_1(x_{t})-V_2(x_t)$$ is a linear combination of the values $$V_1-V_2$$ at other states, weighted by non-negative coefficients whose sum is

$$
\begin{align}
&\sum_{k\ge t}\gamma^{k-t}\mathbb E_\mu\left[\left(\prod_{i=t}^{k-2}c_i\right)(\rho_{k-1}-c_{k-1}\rho_{k})\right]\\\
=&\sum_{k\ge t}\gamma^{k-t}\mathbb E_\mu\left[\left(\prod_{i=t}^{k-2}c_i\right)\rho_{k-1}\right]-\sum_{k\ge t}\gamma^{k-t}\mathbb E_\mu\left[\left(\prod_{i=t}^{k-1}c_i\right)\rho_{k}\right]\\\
&\qquad\color{red}{\text{add }\gamma^{-1}(\mathbb E_{\mu}\rho_{t-1}-1)\text{ to the second term and rearange}}\\\
=&\sum_{k\ge t}\gamma^{k-t}\mathbb E_\mu\left[\left(\prod_{i=t}^{k-2}c_i\right)\rho_{k-1}\right]-\gamma^{-1}\left(\sum_{k\ge t}\gamma^{k-t}\mathbb E_\mu\left[\left(\prod_{i=t}^{k-2}c_i\right)\rho_{k-1}\right]-1\right)\\\
=&\gamma^{-1}-(\gamma^{-1}-1)\sum_{k\ge t}{\gamma^{k-t}\mathbb E_\mu\left[\left(\prod_{i=t}^{k-2}c_i\right)\rho_{k-1}\right]}=\eta\\\
&\qquad\color{red}{\sum_{k\ge t}{\gamma^{k-t}\mathbb E_\mu\left[\left(\prod_{i=t}^{k-2}c_i\right)\rho_{k-1}\right]}\ge\sum_{k=t}^{t+1}{\gamma^{k-t}\mathbb E_\mu\left[\left(\prod_{i=t}^{k-2}c_i\right)\rho_{k-1}\right]}=1+\gamma\mathbb E_\mu\rho_t}\\\
&\qquad\color{red}{\gamma<1\text{ and }(\gamma^{-1}-1)>0}\\\
\le&\gamma^{-1}-(\gamma^{-1}-1)(1+\gamma\mathbb E_\mu\rho_t)\\\
=& 1-(1-\gamma)\mathbb E_\mu\rho_t\\\
&\qquad\color{red}{\mathbb E_\mu\rho_t\ge\beta}\\\
\le& 1-(1-\gamma)\beta\\\
&\qquad\color{red}{\beta\in(0, 1]}\\\
\le&\gamma<1
\end{align}
$$

We deduce that $$\Vert\mathcal RV_1(x_t)-\mathcal RV_1(x_t)\Vert\le \eta\Vert V_1-V_2\Vert_\infty$$, with $$\eta$$ defined in Equation $$\eqref{eq:4}$$, so $$\mathcal R$$ is a contraction mapping. Thus $$\mathcal R$$ possesses a unique fixed point. Let us now prove that this fixed point is $$V^{\pi_\bar\rho}$$. We have

$$
\begin{align}
&\mathbb E_\mu[\rho_t(r_t+\gamma V^{\pi_\bar\rho}(x_{t+1})-V^{\pi_\bar\rho}(x_t))|x_t]\\\
=&\sum_a\mu(a|x_t)\min\left(\bar\rho, {\pi(a|x_t)\over \mu(a|x_t)}\right)\left(r_t+\gamma\sum_{x_{t+1}} p(x_{t+1}|x_t,a)V^{\pi_\bar\rho}(x_{t+1})-V^{\pi_\bar\rho}(x_t)\right)\\\
=&\sum_a\left(r_t+\gamma\sum_{x_{t+1}} p(x_{t+1}|x_t,a)V^{\pi_\bar\rho}(x_{t+1})-V^{\pi_\bar\rho}(x_t)\right)\min\left(\bar\rho\mu(a|x_t), {\pi(a|x_t)}\right)\\\
=&\underbrace{\sum_a\pi_{\bar\rho}(a|x_t)\left(r_t+\gamma\sum_{x_{t+1}} p(x_{t+1}|x_t,a)V^{\pi_\bar\rho}(x_{t+1})-V^{\pi_\bar\rho}(x_t)\right)}_{=0,\text{ since }V^{\pi_\bar\rho}\text{ is the value function of }{\pi_\bar\rho}}\sum_{b\in A}\min(\bar\rho\mu(b|x_t),\pi(b|x_t))\\\
=&0
\end{align}
$$

Therefore $$\delta_kV^{\pi_\bar\rho}=0$$ and $$\mathcal RV^{\pi_\bar\rho}=V^{\pi_\bar\rho}$$, i.e, $$V^{\pi_\bar\rho}v$$ is the unique fixed point of $$\mathcal R$$.

### V-trace Policy Gradient

In the previous subsection, we showed that the value function learned by the V-trace operator converge to $$V^{\pi_{\bar\rho}}$$ other than $$V^*$$. Now we show that even with the optimal value function $$V^*$$, the V-trace policy gradient does not converge to a locally optimal $$\pi^*$$ when data is collected by off policies $$\mu$$

The V-trace policy gradient

$$
\nabla \mathcal J^\mu(\pi)=\mathbb E_{\mu}[\rho_t(r_t+\gamma v(x_{t+1}))\nabla \log\pi(a_t|x_t)]\tag 5
$$

where we omit the baseline and entropy term in the original IMPALA policy gradient.

**Proposition 1.** _The V-trace policy gradient is biased: given the optimal value function $$V^*$$, the V-trace policy gradient does not converge to a locally optimal $$\pi^*$$ for all off-policy behavior distributions $$\mu$$._

**Proof.** We analyze the V-trace policy gradient

$$
\begin{align}
\nabla \mathcal J^\mu(\pi)=&\mathbb E_{\mu}[\rho_t(r_t+\gamma V^*(x_{t+1}))\nabla \log\pi(a_t|x_t)]\\\
=&\mathbb E_{\mu}[\rho_tQ^*(x_t,a_t)\nabla \log\pi(a_t|x_t)]\\\
=&\mathbb E_{\mu}\left[\min\left(\bar\rho,{\pi(a_t|x_t)\over \mu(a_t|x_t)}\right)Q^*(x_t,a_t)\nabla \log\pi(a_t|x_t)\right]\\\
=&\mathbb E_{\mu}\left[{\pi(a_t|x_t)\over \mu(a_t|x_t)}\min\left(1,\bar\rho{\mu(a_t|x_t)\over \pi(a_t|x_t)}\right)Q^*(x_t,a_t)\nabla \log\pi(a_t|x_t)\right]\\\
=&\mathbb E_{\pi}\left[\underbrace{\min\left(1,\bar\rho{\mu(a_t|x_t)\over \pi(a_t|x_t)}\right)}_{\omega(x_t,a_t)}Q^*(x_t,a_t)\nabla \log\pi(a_t|x_t)\right]\\\
=&\mathbb E_{\pi}\left[Q^\omega(x_t,a_t)\nabla \log\pi(a_t|x_t)\right]\tag 6
\end{align}
$$

when $$\bar\rho\mu(a_t\vert x_t)< \pi(a_t\vert x_t)$$, $$\omega(x_t,a_t)<1$$ and thus the policy gradient is biased. More specifically, when $$\bar\rho\mu(a_t\vert x_t)< \pi(a_t\vert x_t)$$, the V-trace policy gradient penalizes the action values by weighting $$Q^*(x_t,a_t)$$ with $$\omega(x_t,a_t)$$. From the probabilistic view, this changes the policy from one proportional to $$\exp(Q^*)$$ to one proportional to $$\exp(Q^\omega)$$. In the greedy case, it may or may not change the optimal policy depending on how much $$Q^\omega$$ is distorted from $$Q^*$$: regrets are introduced as long as there exists some $$x$$ such that $$\arg\max_a(Q^\omega(x,a))\ne\arg\max(Q^*(x,a))$$.

### Mixing On- and Off-Policy Experiences

In Equation $$\eqref{eq:6}$$, we showed that the V-trace policy gradient is biased when $$\bar\rho\mu(a_t\vert x_t)< \pi(a_t\vert x_t)$$. We now show that by mixing the on- and off-policy data, the bias can be reduced. Furthermore, it is possible to select a proportion of on-policy data so that the learned policy is unbiased in the greedy case. 

**Proposition 2.** *Mixing on-policy data into the V-trace policy gradient with the ratio $$\alpha$$ reduces the bias by providing a regularization to the implied state-action values. In the general function approximation case it changes the off-policy V-trace policy gradient from $$\sum_x d^\mu(x)\mathbb E_\pi[(Q(x, a)\nabla\log\pi(a\vert x)]$$ to $$\sum_x\mathbb E_\pi [Q^\alpha(x, a)\nabla\log\pi(a\vert x)]$$ where $$Q^\alpha = Qd^\pi(x)\alpha + Q^\omega d^\mu(x)(1 − \alpha)$$ is a regularized state action estimate and $$d^\pi$$ , $$d^\mu$$ are the state distributions for $$\pi$$ and $$\mu$$ . Note that there exists $$\alpha\le 1$$ such that $$Q^\alpha$$ has the same argmax (i.e. best action) as $$Q$$.*

**Proof.** Note that the on-policy policy gradient is given by

$$
\nabla \mathcal J^\pi(\pi)=\sum_xd^\pi(x)\mathbb E_{\pi}[Q(x,a)\nabla \log\pi(a|x)]\tag 7
$$

Similarly the off-policy V-trace gradient is given by

$$
\nabla \mathcal J^\mu(\pi)=\sum_xd^\mu(x)\mathbb E_{\pi}[\omega(x,a)Q(x,a)\nabla \log\pi(a|x)]\tag 8
$$

The $$\alpha$$-interpolation of both gradients can be transformed as follows:

$$
\begin{align}
\nabla(\alpha\nabla \mathcal J^\pi+(1-\alpha)\nabla\mathcal J^\mu)(\pi)=&\alpha\sum_xd^\pi(x)\mathbb E_{\pi}[Q(x,a)\nabla \log\pi(a|x)]\\\
&+(1-\alpha)\sum_xd^\mu(x)\mathbb E_{\pi}[\omega(x,a)Q(x,a)\nabla \log\pi(a|x)]\\\
=&\sum_x\mathbb E_\pi\Big[\big(Q(x,a)d^\pi(x)\alpha
+Q^\omega(x,a)d^\mu(x)(1-\alpha)\big)\nabla\log\pi(a|x)\Big]\\\
=&\sum_x\mathbb E_\pi [Q^\alpha(x, a)\nabla\log\pi(a|x)]
\end{align}
$$


for $$Q^\alpha(x,a) = Q(x,a)d^\pi(x)\alpha + Q^\omega(x,a) d^\mu(x)(1 − \alpha)$$.

**Interpretation.** We show that in the greedy sense, choosing $$\alpha$$ such that

$$
{\alpha\over1-\alpha}>\max_{b\ne A^*}\left[{Q^\omega(x,b)-Q^\omega(x,a^*)\over Q(x,a^*)-Q(x,b)}\right]{d^\mu(x)\over d^\pi(x)}\tag 9
$$

will resulting in a policy that produces the same optimal action as the optimal policy, i.e.,

$$
\arg\max_a[Q(x,a)]=\arg\max_a[Q^\alpha(x,a)]\quad \forall x \tag 10
$$

Let $$a^*=\arg\max_a Q(x,a)$$ be the best action and $$A^*$$ the set of best actions. Equation $$\eqref{eq:9}$$ is equivalent to

$$
Q^\alpha(x,a^*)>Q^\alpha(x,b)\quad\forall b\notin A^*
$$

By the definition of $$Q^\alpha$$, we have

$$
\begin{align}
Q(x,a^*)d^\pi(x)\alpha + Q^\omega(x,a^*) d^\mu(x)(1 − \alpha)&>Q(x,b)d^\pi(x)\alpha + Q^\omega(x,b) d^\mu(x)(1 − \alpha)\quad\forall b\notin A^*\\\
\big(Q(x,a^*)d^\pi(x)-Q(x,b)d^\pi(x)\big)\alpha&>\big(Q^\omega(x,b) d^\mu(x)-Q^\omega(x,a^*) d^\mu(x)\big)(1 − \alpha)\quad\forall b\notin A^*\\\
{\alpha\over1-\alpha}&>\max_{b\ne A^*}\left[{Q^\omega(x,b)-Q^\omega(x,a^*)\over Q(x,a^*)-Q(x,b)}\right]{d^\mu(x)\over d^\pi(x)}
\end{align}
$$

This inequality provides us several interesting observations

1. The larger the action value gap in the real $$Q$$-function $$Q(s,a^*)-Q(s,b)$$, the less on-policy data is required
2. If $$\max_{b\ne A^*}Q^\omega(x,b)-Q^\omega(x,a^*)$$ is negative, $$\alpha$$ may be as small as zero. That is, when $$Q^\omega$$ does not have the same optimal action as $$Q$$, the proportional of on-policy data does not matter. (What!?)
3. Less on-policy data is required if $$d^\mu(x)\over d^\pi(x)$$ is small, i.e. if $$\pi$$ visits state $$x$$ more often than $$\mu$$.

## References

<a name='ref1'></a>Espeholt, Lasse, Hubert Soyer, Remi Munos, Karen Simonyan, Volodymyr Mnih, Tom Ward, Boron Yotam, et al. 2018. “IMPALA: Scalable Distributed Deep-RL with Importance Weighted Actor-Learner Architectures.” *35th International Conference on Machine Learning, ICML 2018* 4: 2263–84.

<a name='ref2'></a>Schmitt, Simon, Matteo Hessel, and Karen Simonyan. 2019. “Off-Policy Actor-Critic with Shared Experience Replay.” *ArXiv*, no. Figure 2: 1–20.