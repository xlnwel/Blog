---
title: "SAC — Soft Actor-Critic"
excerpt: "In which we talk about soft actor-critic, a maximum entropy algorithm."
categories:
  - Reinforcement Learning
tags:
  - Reinforcement Learning
  - Model-Free Reinforcement Learning
---

## Introduction

Model-free reinforcement learning algorithms oftentimes suffer the problem of changes in environment, especially after the algorithm converges, which is generally caused by lack of exploration. In the previous posts, we have discussed a probabilistic graphical model that allows us to maximize the policy entropy at the same time maximizing the expected reward. Maximum entropy policies provide a substantial improvement in exploration and are robust in face of model and estimation errors. In this post, we will continue our discussion by blending in neural networks, which yields an off-policy algorithm named Soft Actor-Critic(SAC). 

## Preliminaries

Back when we talked about [Soft Value Iteration algorithm]({{ site.baseurl }}{% post_url 2019-01-21-SVI %}), we learnt that maximum entropy RL maximized the policy entropy as well as cumulative rewards:

$$
\mathcal J(q)=\sum_{t=1}^T\gamma^{t-1}\mathbb E_{s_{t}, a_{t}\sim q}\left[r(s_t,a_t)+\alpha\mathcal H(q(a_t|s_t))\right]
$$

Notice that this objective differs from the behavior of Boltzmann exploration widely used in policy gradient method: It maximizes action entropy at states instead of at the current step, encouraging the agent to reach states with high entropy in the future.

With the above objective, we derived the following update rules

$$
\begin{align}
Q(s_t,a_t)&=r(s_t,a_t)+\gamma\mathbb E_{p(s_{t+1}|s_t,a_t)}[V(s_{t+1})]\tag{1}\label{eq:1}\\\
V(s_t)&=\max_{\pi(a_t|s_t)} \mathbb E_{a_{t}\sim \pi(a_t|s_t)}[Q(s_t,a_t)+\alpha\mathcal H(\pi(a_t|s_t))]\tag{2}\label{eq:2}\\\
\pi(a_t|s_t)&=\exp\left({1\over\alpha}\Big(Q(s_t,a_t)-V(s_t)\Big)\right)\tag{3}\label{eq:3}
\end{align}
$$

These rules not only pave a way to construct the dynamic programming algorithm, but also provide the targets for respective function approximators.

## Soft Actor-Critic

Soft Actor-Critic (SAC) is comprised of five networks(compared to six networks in TD3, this is actually not that kind of many…But it has four networks to perform gradient descent, which makes it more computationally expensive): two state value functions $$V_\psi$$ and $$V_{\bar\psi}$$, two action value functions $$Q_{\theta_1}$$ and $$Q_{\theta_2}$$, and a policy function $$\pi_\phi$$. The weights of the target network $$V_{\bar\psi}$$ could either be the exponential moving average of $$V_\psi$$ or be periodically updated to the weights of $$V_\psi$$. $$Q_{\theta_1}$$ and $$Q_{\theta_2}$$ are trained simultaneously with the same objective to alleviate the overestimation bias in $$Q$$ networks as TD3. We do not have target networks for $$Q$$ functions since they are updated based on the target $$V$$. Therefore, SAC has three bobjectives in total. Next, we will address them one by one and relates them to Eqs.$$(1-3)$$.

For the action value function $$Q_{\theta}$$, we take Eq.$$\eqref{eq:1}$$ as the target value, and have the mean square error

$$
\mathcal L_Q(\theta_i)=\mathbb E_{s_t,a_t, s_{t+1}\sim \mathcal D}\left[\big(r(s_t,a_t)+\gamma V_{\bar\psi}(s_{t+1})-Q_{\theta_i}(s_t,a_t)\big)^2\right]\quad \mathrm{for}\ i\in\{1,2\}\tag{4}\label{eq:4}
$$

Just as TD3, we take $$Q_{\theta}=\min(Q_{\theta_1}, Q_{\theta_2})$$ in the following objectives to mitigate the overestimation bias.

For the state value function $$V_\psi$$, we take Eq.$$\eqref{eq:2}$$ as the target value, and whereby we minimize the mean square error

$$
\mathcal L_V(\psi)=\mathbb E_{s_t\sim \mathcal D}\left[\big(\mathbb E_{a_t\sim \pi_\phi(a_t|s_t)}[Q_\theta(s_t,a_t)-\alpha\log \pi_\phi(a_t|s_t)]-V_\psi(s_t)\big)^2\right]\tag{5}\label{eq:5}
$$

For the policy function $$\pi_{\phi}$$, we take Eq.$$\eqref{eq:3}$$ as the objective, and minimize the KL divergence

$$
\mathcal L_\pi(\phi)=\mathbb E_{s_t\sim \mathcal D}\left[D_{KL}\Bigg(\pi_\phi(\cdot|s_t)\Bigg\Vert \exp\left({1\over\alpha}\Big(Q_\theta(s_t,\cdot)-V_\psi(s_t)\Big)\right)\Bigg)\right]\tag{6}\label{eq:6}
$$

In addition, we could further simplify the policy loss using the reparameterization trick since the $$Q$$-function represented by a neural network is continuous: let $$a_t=f_\phi(\epsilon; s_t)$$, then Eq.$$\eqref{eq:6}$$ could be simplified as

$$
\begin{align}
\mathcal L_\pi(\phi)&=\mathbb E_{s_t\sim \mathcal D, \epsilon_t\sim \mathcal N}\left[\log \pi_\phi(f_\phi(\epsilon_t;s_t)|s_t)-{1\over\alpha}Q_\theta(s_t,f_\phi(\epsilon_t;s_t))\right]\tag{7}\label{eq:7}\\\
&\propto\mathbb E_{s_t\sim \mathcal D, \epsilon_t\sim \mathcal N}\left[\alpha\log \pi_\phi(f_\phi(\epsilon_t;s_t)|s_t)-Q_\theta(s_t,f_\phi(\epsilon_t;s_t))\right]\tag{8}\label{eq:8}
\end{align}
$$


where in the first step we omit $$V_\psi(s_t)$$ since it is not related to action selection. Note that this is not the case in conventional policy gradient methods whose objective is $$\sum_t r(s,a)$$. $$\sum_t r(s,a)$$ cannot be reparametrized because in general we do not have the access to the reward function.

## Notes

There are some notes for SAC in practice:

1. Eq.$$\eqref{eq:8}$$ is generally used as the loss for the policy network. This also helps reduce the step-size of the policy.

2. Because the algorithm maximizes trade-off of reward and entropy at every state, entropy must be unique to state — and therefore the logarithm of the standard deviation need to be the output of a neural network instead of a shared-across states learnable parameter vector. But for deep networks, especially those use Relu, simply sticking an activationless dense layer at the end would be quite bad — at the beginning of training, a randomly initialized net could produce extremely large values for the log of the standard deviation, which would result in some actions being either entirely deterministic or too random to come back to earth. Either of these introduces numerical instability which could break the algorithm. To protect agains that, we'll constrain the output of the logarithm of the standard deviation, e.g. to lie within $$[-20, 2]$$.

3. It may be more desirable to use bounded actions. To that end, we apply an invertible squashing function $$\tanh$$ to the Gaussian samples, and employ the [change of variables formula](https://en.wikipedia.org/wiki/Integration_by_substitution#Substitution_for_multiple_variables) to compute the likelihoods of the bounded action. This changes the density to
   
$$
   \pi(a|s)=\mu(u|s)\left|\det\left(da\over du\right)\right|^{-1}
   $$

   where $$\mu(u\vert s)$$ is the diagonal Gaussian distribution produced by the neural network before squashing. Because the Jacobian $$da/du=\mathrm{diag}(1-\tanh^2(u))$$ is diagonal, the log-likelihood has a simple form
   
$$
   \begin{align}
   \log\pi(a|s)&=\log\mu(u|s)-\sum_{i=1}^D\log(1-\tanh^2(u_i))\\\
   &=\log\mu(u|s)-\sum_{i=1}^D2\left(\log2-u_i-\mathrm{softplus}(-2u_i)\right)\tag{9}\label{eq:9}
   \end{align}
   $$

   where $$\mu_i$$ is the $$i^{th}$$ element of $$u$$. 

4. When $$\alpha\rightarrow 0$$, $$V$$'s target becomes $$Q$$ and thus could be omitted. Furthermore, the first term in Eq.$$\eqref{eq:7}$$ is overwhelmed by the second and therefore could be ignored. As a result, The objectives in SAC become identical to those in TD3.

## Experimental Results

<figure>
  <img src="{{ '/images/soft optimality/sac-exp-results.png' | absolute_url }}" alt="" width="1000">
  <figcaption>Learning curves on continuous control benchmarks</figcaption>
  <style>
    figure figcaption {
    text-align: center;
    }
  </style>
</figure>

## Ablation Study

Here we list several interesting experimental results on SAC

1. Take the policy mean as action at evaluation time improves the performance
2. SAC is particularly sensitive to the scaling of the reward signal, because it serves as the temporature of the energy-based optimal policy and thus controls its stochasticity (see Eq.$$\eqref{eq:6}$$).
3. The authors found that, compared to exponentially moving average, SAC benefits from updating the target value network every 1000 gradient steps when taking more than one gradient steps between environment steps.

## Reference

Tuomas Haarnoja et al. Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor

OpenAI Spinning Up: [Soft Actor-Critic](https://spinningup.openai.com/en/latest/algorithms/sac.html)
