---
title: "Retrace(ùùÄ)"
excerpt: "In which we theoretically analyze the Retrace(ùùÄ) algorithm."
categories:
  - Reinforcement Learning
tags:
  - Reinforcement Learning
  - Mathematics
---

## Introduction

Consider the general operator for return-based off-policy algorithms:

$$
\mathcal RQ(x_t,a_t):=Q(x_t,a_t)+\mathbb E_\mu\left[\sum_{k=t}^{t+n-1}\gamma^{k-t}\left(\prod_{i=t+1}^k c_i\right)(r_k+\gamma\mathbb E_{\pi} Q(x_{k+1},\cdot)-Q(x_k,a_k))\right]\tag{1}\label{eq:1}
$$

where $$\pi$$ and $$\mu$$ are the target and behavior policies, respectively. All $$Q$$s are computed from the target network. Munos et al. introduced Retrace($$\lambda$$), which defines $$c_i=\lambda\min\left(1,{\pi(a_i\vert x_i)\over\mu(a_i\vert x_i)}\right)$$, with the following advantages:

- Retrace($$\lambda$$) truncates the IS ratio at $$1$$, whereby it does not suffering from the variance explosion due to the product of IS ratios.
- It does not cut the traces in the on-policy case, making it possible to benefit from the full returns
- In the off-policy case, the traces are safely cut: they guarantee the convergence for arbitrary $$\pi$$ and $$\mu$$.

In the following post, we will show the following:

- For any traces $$0\le c\le{\pi(a_t\vert x_t)\over\mu(a_t\vert x_t)}$$, the return-based operator(Equation $$\eqref{eq:1}$$) is a $$\gamma$$-contraction around $$Q^\pi$$, for arbitrary policies $$\mu$$ and $$\pi$$
- In the control case (where $$\pi$$ is replaced by a sequence of increasingly greedy policies) the online Retrace($$\lambda$$) algorithm converges a.s.(almost surely) to $$Q^*$$, without requiring the GLIE assumption(Greedy in the Limit with Infinite Exploration)

## Analysis of Retrace($$\lambda$$)

### Policy Evaluation

In the evaluation setting, we prove that $$\mathcal R$$ operator defined in Equation $$\eqref{eq:1}$$ is $$\gamma$$-contraction around its fixed point $$Q^\pi$$ for any traces $$0\le c\le{\pi(a_t\vert x_t)\over\mu(a_t\vert x_t)}$$. We first introduce the following lemma.

<a name='lemma1'></a>**Lemma 1. **The difference between $$\mathcal RQ$$ and its fixed point $$Q^\pi$$ is

$$
\begin{align}
\mathcal RQ(x_t,a_t)-Q^\pi(x_t,a_t)=\mathbb E_\mu\left[\sum_{k\ge t+1}\gamma^{k-t}\left(\prod_{i=t+1}^{t-1}c_i\right)\left(\mathbb E_\pi(Q-Q^\pi)(x_k,\cdot)-c_k(Q-Q^\pi)(x_k,a_k)\right)\right]\tag{2}\label{eq:2}
\end{align}
$$

**Proof. **We begin by rewriting Equation $$\eqref{eq:1}$$:

$$
\mathcal RQ(x_t,a_t)=\sum_{k\ge t}\gamma^{k-t}\mathbb E_\mu\left[\left(\prod_{i=t+1}^kc_i\right)\left(r_t+\gamma\big[\mathbb E_\pi Q(x_{k+1},\cdot)\big]-c_{k+1}Q(x_{k+1},a_{k+1})\right)\right]\tag{3}\label{eq:3}
$$

where we move $$Q(x_{k+1})$$ in $$\delta_{k+1}Q$$ into $$\delta_{k}Q$$,. Since $$Q^\pi$$ is the fixed point of $$\mathcal R$$, we have

$$
Q^\pi=\mathcal RQ^\pi(x_t,a_t)=\sum_{k\ge t}\gamma^{k-t}\mathbb E_\mu\left[\left(\prod_{i=t+1}^kc_i\right)\left(r_t+\gamma\big[\mathbb E_\pi Q^\pi(x_{k+1},\cdot)\big]-c_{k+1}Q^\pi(x_{k+1},a_{k+1})\right)\right]\tag4
$$

We now can deduce Lemma 1 by subtracting Equation $$\eqref{eq:4}$$ from Equation $$\eqref{eq:3}$$. 

In the rest of the discussion, all expectations are taken over (potentially a sequence of behavior policies) $$\mu$$ if the distribution is not explicitly specified.

Define history $$\mathcal F_i$$ as sequences drawn from a behavior policy $$\mu$$ up to time step $$i$$, non-negative coefficients $$c_i=c_i(a_i,\mathcal F_i)$$ as a function of $$a_i$$ and $$\mathcal F_i$$. Next, we show $$\mathcal R$$ is $$\gamma$$-contraction around $$Q^\pi$$, i.e., $$\Vert \mathcal RQ-Q^\pi\Vert\le\gamma\Vert Q-Q^\pi\Vert$$, where $$\Vert\cdot\Vert$$ is supremum norm. 

**Theorem 1.** *The operator $$\mathcal R$$ defined by Equation $$\eqref{eq:1}$$ has a fixed point $$Q^\pi$$. Furthermore, if for each $$a_i\in\mathcal A$$ and each history $$\mathcal F_i$$ we have $$c_i\in\left[0,{\pi(a_i\vert x_i)\over \mu(a_i\vert x_i)}\right]$$, then for any $$Q$$-function $$Q$$*

$$
\Vert \mathcal RQ-Q^\pi\Vert\le\gamma\Vert Q-Q^\pi\Vert
$$

 $$\mu$$. From Lemma [1](#lemma1) and defining $$\Delta Q:=Q-Q^\pi$$, we have

$$
\begin{align}
\mathcal RQ(x_t,a_t)-Q^\pi(x_t,a_t)=&\sum_{k\ge t+1}\gamma^{k-t}\mathbb E_{x_{t:k},a_{t:k}}\left[\left(\prod_{i=t+1}^{k-1}c_i\right)\left(\mathbb E_\pi\Delta Q(x_k,\cdot)-c_k\Delta Q(x_k,a_k)\right)\right]\\\
&\qquad\color{red}{\text{move }\mathbb E_{a_k\sim\mu}\text{ inward}}\\\
=&\sum_{k\ge t+1}\gamma^{k-t}\mathbb E_{x_{t:k},a_{t:k-1}}\left[\left(\prod_{i=t+1}^{k-1}c_i\right)\left(\sum_b\pi(b|x_k)\Delta Q(x_k,b)-\mu(b|x_k)c_k\Delta Q(x_k,b)\right)\right]\\\
=&\sum_{k\ge t+1}\gamma^{k-t}\mathbb E_{x_{t:k},a_{t:k-1}}\left[\left(\prod_{i=t+1}^{k-1}c_i\right)\left(\sum_b\big(\pi(b|x_k)-\mu(b|x_k)c_k\big)\Delta Q(x_k,b)\right)\right]\\\
\end{align}
$$


As $$\pi(b\vert x_k)-\mu(b\vert x_k)c_k\ge0$$, we have $$\mathcal RQ(x_t,a_t)-Q^\pi(x_t,a_t)=\sum_{x_t,b}w_{x_t,b}\Delta Q(x_t,b)$$, where coefficients $$w_{x_t,b}$$ is defined as

$$
w_{x_t,b}=\sum_{k\ge t+1}\gamma^{k-t}\mathbb E_{x_{t:k},a_{t:k-1}}\left[\left(\prod_{i=t+1}^{k-1}c_i\right)\big(\pi(b|x_k)-\mu(b|x_k)c_k\big)\right]
$$

The sum of those coefficients is

$$
\begin{align}
\sum_{x_t,b}w_{x_t,b}=&\sum_{k\ge t+1}\gamma^{k-t}\mathbb E_{x_{t:k},a_{t:k-1}}\left[\left(\prod_{i=t+1}^{k-1}c_i\right)\left(\sum_b\big(\pi(b|x_k)-\mu(b|x_k)c_k\big)\right)\right]\\\
=&\sum_{k\ge t+1}\gamma^{k-t}\mathbb E_{x_{t:k},a_{t:k-1}}\left[\left(\prod_{i=t+1}^{k-1}c_i\right)\big(1-\mathbb E_{a_k}c_k\big)\right]\\\
=&\sum_{k\ge t+1}\gamma^{k-t}\mathbb E_{x_{t:k},a_{t:k}}\left[\prod_{i=t+1}^{k-1}c_i-\prod_{i=t+1}^{k}c_i\right]\\\
=&\gamma\sum_{k\ge t}\gamma^{k-t}\mathbb E_{x_{t:k},a_{t:k}}\prod_{i=t+1}^{k}c_i-\left(\sum_{k\ge t}\gamma^{k-t}\mathbb E_{x_{t:k},a_{t:k}}\prod_{i=t+1}^{k}c_i-\gamma^0\prod_{i=t+1}^tc_i\right)\\\
&\qquad\color{red}{C=\sum_{k\ge t}\gamma^{k-t}\mathbb E_{x_{t:k},a_{t:k}}\prod_{i=t+1}^{k}c_i}\\\
=&\gamma C-(C-1)
\end{align}
$$

Because $$C-1\ge0$$, we have $$\gamma C-\gamma-(C-1)=(\gamma-1)(C-1)\le 0$$, i.e. $$\sum_{x_t,b}w_{x_t,b}\le\gamma$$. Thus $$\mathcal R Q(s,a)-Q^\pi(s,a)$$ is a sub-convex combination of $$\Delta Q$$ weighted by non-negative coefficients $$w_{s,b}$$ which sum to at most $$\gamma$$, thus $$\mathcal R$$ is a $$\gamma$$-contraction mapping around $$Q^\pi$$.

**Remark 1.** Notice that the coefficient $$C$$ above depends on $$(x, a)$$. If we write $$\eta(x,a)=1-(1-\gamma)C$$, then we have shown that

$$
\Vert \mathcal RQ(x,a)-Q^\pi(x,a)\Vert\le\eta(x,a)\Vert Q-Q^\pi\Vert
$$

Thus $$\eta(x,a)\in[0,\gamma]$$ is a $$(x,a)$$-specific contraction coefficient, which is $$\gamma$$ when $$c_1=0$$ (the trace is cut immediately) and can be close to zero when learning from full returns $$c_k\approx1$$ for all $$k\ge t$$.

### Control

In the control setting, the single target policy $$\pi$$ is replaced by a sequence of policies $$\pi_k$$ which depends on $$Q_k$$. We first define a sequence of increasingly greedy policies

**Definition 1.** A sequence of policies ($$\pi_k:k\in \mathbb N$$) is increasingly greedy w.r.t. a sequence ($$Q_k:k\in\mathbb N$$) of $$Q$$-functions if the following property holds for all $$k$$: $$P^{\pi_{k+1}}Q_{k+1}\ge P^{\pi_k}Q_{k+1}$$, where $$P^\pi Q(x,a)=\sum_{x'}\sum_{a'}P(x'\vert x,a)\pi(a'\vert x')Q(x',a')$$.

Intuitively, this means that each $$\pi_{k+1}$$ is at east as greedy as the previous policy $$\pi_k$$ for $$Q_{k+1}$$. Many natural sequences of policies are increasingly greedy, including $$\epsilon_k$$ greedy policy (with non-increasing $$\epsilon_k$$) and softmax policies (with non-increasing temperature).

Notice that, in the Retrace algorithm, $$c_i$$ is Markovian, in the sense that it depends on $$x_i, a_i$$ (as well as $$\pi$$ and $$\mu$$) only but not on the full past history. This allows us to define the (sub)-probability

$$
(P^{c\mu}Q)(x,a)=\sum_{x'}\sum_{a'}P(x'|x,a)\mu(a'|x')c(x',a')Q(x',a')\tag 5
$$

With help of Equation $$\eqref{eq:5}$$, we rewrite the state-based Retrace operator as follow

$$
\begin{align}
\mathcal RQ=&Q+\sum_{t\ge 0}\gamma^t(P^{c\mu})^t(\mathcal T^\pi Q-Q)\tag 6\\\
=&Q+(I-\gamma P^{c\mu})^{-1}(\mathcal T^\pi Q-Q)\tag 7\\\
where\quad \mathcal T^\pi Q=&r+P^\pi Q'
\end{align}
$$

where we omit $$x,a$$ for simplicity. Notice that the Retrace operator defined by Equations $$(6-7)$$ is state based while the one defined by Equation $$\eqref{eq:1}$$ is time-based.

**Theorem 2.** *Consider an arbitrary sequence of behaviour policies ($$\mu_k$$) (which may depend on ($$Q_k$$)) and a sequence of target policies ($$\pi_k$$) that are increasingly greedy w.r.t. the sequence ($$Q_k$$):*

$$
Q_{k+1}=\mathcal R_k Q_k
$$

_Assume the target policies $$\pi_k$$ are $$\epsilon_k$$-away from the greedy policies w.r.t. $$Q_k$$, in the sense that $$\mathcal T^{\pi_k}Q_k\ge\mathcal T^* Q_k-\epsilon_k\Vert Q_k\Vert e$$, where $$e$$ is the vector with $$1$$-components. Further suppose that $$\mathcal T^{\pi_0}Q_0\ge Q_0$$. Then for any $$k\ge 0$$,_

$$
\Vert Q_{k+1}-Q^*\Vert\le\gamma \Vert Q_{k}-Q^*\Vert+\epsilon_k\Vert Q_k\Vert
$$

_In consequence, if $$\epsilon_k\rightarrow 0$$ then $$Q_k\rightarrow Q^*$$._

**Proof.** We now lower- and upper-bound the term $$Q_{k+1}- Q^*$$

**Upper bound on $$Q_{k+1}- Q^*$$**. Since $$Q_{k+1}=\mathcal R_k Q_k$$, we have

$$
\begin{align}
Q_{k+1}- Q^*=&Q_k+(I-\gamma P^{c\mu_k})^{-1}(\mathcal T^{\pi_k} Q_k-Q_k)-Q^*\\\
=&(I-\gamma P^{c\mu_k})^{-1}\big(\mathcal T^{\pi_k} Q_k-Q_k+(I-\gamma P^{c\mu_k})(Q_k-Q^*)\big)\\\
=&(I-\gamma P^{c\mu_k})^{-1}\big(\mathcal T^{\pi_k} Q_k-Q^*+\gamma P^{c\mu_k}(Q_k-Q^*)\big)\\\
=&(I-\gamma P^{c\mu_k})^{-1}\big(\mathcal T^{\pi_k} Q_k-\mathcal TQ^*+\gamma P^{c\mu_k}(Q_k-Q^*)\big)\\\
&\qquad \color{red}{\mathcal T^{\pi_k} Q_k-\mathcal TQ^*=\gamma P^{\pi_k}Q_k-\gamma PQ^*\le \gamma P^{\pi_k}(Q_k-Q^*)}\\\
\le &(I-\gamma P^{c\mu_k})^{-1}\big(\gamma P^{\pi_k}(Q_k-Q^*)+\gamma P^{c\mu_k}(Q_k-Q^*)\big)\\\
=&\gamma(I-\gamma P^{c\mu_k})^{-1}(P^{\pi_k}-P^{c\mu_k})(Q_k-Q^*)\\\
&\qquad \color{red}{A_k=\gamma(I-\gamma P^{c\mu_k})^{-1}(P^{\pi_k}-P^{c\mu_k})}\\\
=&A_k(Q_k-Q^*)
\end{align}
$$


It is clear that $$A_k$$ has non-negative elements as

$$
(P^{\pi_k}-P^{c\mu})e(x,a)=\sum_{x'}\sum_{a'}P(x'|x,a)(\pi(a'|x')-c(x',a')\mu_k(a'|x'))\ge 0
$$

By rewriting $$A_k$$ as $$\gamma\sum_{t\ge0}\gamma^t(P^{c\mu_k})^t(P^{\pi_k}-P^{c\mu_k})$$, we have

$$
\begin{align}
A_ke=&\gamma\sum_{t\ge0}\gamma^t(P^{c\mu_k})^t(P^{\pi_k}-P^{c\mu_k})e\\\
&\qquad\color{red}{P^{\pi}e=1}\\\
=&\gamma\sum_{t\ge0}\gamma^t(P^{c\mu_k})^te-\sum_{t\ge0}\gamma^{t+1}(P^{c\mu_k})^{t+1}e\\\
&\qquad\color{red}{\sum_{t\ge0}\gamma^{t+1}(P^{c\mu_k})^{t+1}e=\sum_{t\ge0}\gamma^{t}(P^{c\mu_k})^{t}e-e}\\\
=&(\gamma-1)\sum_{t\ge0}\gamma^{t}(P^{c\mu_k})^{t}e+e\\\
&\qquad\color{red}{\sum_{t\ge0}\gamma^{t}(P^{c\mu_k})^{t}e\ge e}\\\
\le&\gamma e
\end{align}
$$

Therefore, $$A_k$$ has non-negative elements, whose sum over each row (all $$(x,a)$$) is at most $$\gamma$$. We deduce that $$Q_{k+1}-Q^*\le \gamma\Vert Q_k-Q^*\Vert e$$.

**Lower bound on $$Q_{k+1}-Q^*$$**. For this part, we refer interested readers to the paper as it's already clean.

## References

Munos, R√©mi, Tom Stepleton, Anna Harutyunyan, and Marc G. Bellemare. 2016. ‚ÄúSafe and Efficient Off-Policy Reinforcement Learning.‚Äù *NIPS 2016*, November. http://arxiv.org/abs/1606.02647.

Richard S. Sutton and Andrew G. Barto. 2018. Reinforcement Learning: An Introduction

## Supplementary Materials

We can derive Equation 1 from the action values with control variates(Sutton & Barto 2018)

$$
q(x_t,a_t)=r_t+\gamma\big(c_{t+1}q(x_{t+1},a_{t+1})+ V(x_{t+1})-c_{t+1}Q(x_{t+1},a_{t+1})\big)\\\
where\quad V(x_t)=\mathbb E_{a\sim\pi(a|s)}[Q(x_t,a_t)]
$$

where $$q$$ denotes the target, $$Q$$ is the action value function. Expanding $$q(x_{t+1},a_{t+1})$$, we will have

$$
\begin{align}
q(x_t,a_t)=&r_t+\gamma V(x_{t+1})-Q(x_t,a_t)+Q(x_t,a_t)+\gamma c_{t+1}\big(q(x_{t+1},a_{t+1})-Q(x_{t+1},a_{t+1})\big)\\\
=&\delta_t+Q(x_t,a_t)+\gamma c_{t+1}(\delta_{t+1}+\gamma c_{t+2}(q(x_{t+2},a_{t+2})-Q(x_{t+2},a_{t+2})))\\\
=&Q(x_t,a_t)+\sum_{k=t}^{t+n-1}\gamma^{k-t}\prod_{i=t+1}^kc_i\delta_k
\end{align}
$$

where $$\delta_k=r_t+\gamma V(x_{t+1})-Q(x_t,a_t)$$ and we take $$q(x_{t+n}, a_{t+n})=Q(x_{t+n}, a_{t+n})$$ at the last step. We conclude $$q(x_t,a_t)=\mathcal RQ(x_t,a_t)$$

We now derive a recursive computation for Retrace($$\lambda$$)

$$
\begin{align}
q(x_t,a_t)=&Q(x_t,a_t)+r_t+\gamma  V(x_{t+1})-Q(x_t,a_t)+\sum_{k=t+1}^{t+n-1}\gamma^{k-t}\prod_{i=t+1}^kc_i\delta_k\\\
=&r_t+\gamma  V(x_{t+1})-\gamma c_{t+1}Q(x_{t+1},a_{t+1}) + \gamma c_{t+1}\underbrace{\left(Q(x_{t+1},a_{t+1})+\sum_{k=t+1}^{t+n-1}\gamma^{k-(t+1)}\prod_{i=t+2}^kc_i\delta_k\right)}_{q(x_{t+1},a_{t+1})}\\\
=&r_t+\gamma (V(x_{t+1})-c_{t+1}Q(x_{t+1},a_{t+1})) +\gamma c_{t+1}q(x_{t+1},a_{t+1})
\end{align}
$$

Alternatively, we write

$$
\begin{align}
q(x_t,a_t)=&Q(x_t,a_t)+\sum_{k=t}^{t+n-1}\gamma^{k-t}\prod_{i=t+1}^kc_i\delta_k\\\
=&Q(x_t,a_t)+\delta_t+\gamma c_{t+1}\sum_{k=t+1}^{t+n-1}\gamma^{k-t-1}\prod_{i=t+2}^kc_i\delta_k\\\
q(x_t,a_t)-Q(x_t,a_t)=&\delta_t+\gamma c_{t+1}\big(q(x_{t+1},a_{t+1})-Q(x_{t+1}, a_{t+1})\big)
\end{align}
$$

