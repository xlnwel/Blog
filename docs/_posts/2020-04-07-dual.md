---
title: "Duality in Linear Programm"
excerpt: "An introduction to dual linear programs"
categories:
  - Mathematics
tags:
  - Mathematics
---

## Introduction

For a primal linear program defined by

$$
\begin{align}
&\text{maximize}&\pmb c^\top \pmb x\\\
&s.t. &\pmb A\pmb x\le \pmb b\\\
&&\pmb x\ge\pmb 0
\end{align}
$$

the dual takes the form

$$
\begin{align}
&\text{minimize}&\pmb b^\top \pmb y\\\
&s.t. &\pmb A^\top \pmb y\ge \pmb c\\\
&&\pmb y\ge\pmb 0
\end{align}
$$

This is called the symmetric dual problem. There are two other dual problems listed [here](https://en.wikipedia.org/wiki/Dual_linear_program#Vector_formulations). For simplicity, we only discuss the symmetric dual problem.

## Weak Duality

**Theorem 1.** For any feasible solution $$\pmb x$$ and $$\pmb y$$ to primal and dual linear programs, $$\pmb c^\top \pmb x\le \pmb b^\top \pmb y$$.

**Proof:** Multiplying both sides of $$\pmb A\pmb x\le \pmb b$$ by $$\pmb y^\top\ge \pmb 0$$, we have $$\pmb y^\top \pmb A\pmb x\le \pmb y^\top \pmb b$$. Since $$\pmb A^\top \pmb y\ge \pmb c$$, we have $$\pmb c^\top \pmb x\le \pmb y^\top \pmb A\pmb x\le \pmb y^\top \pmb b$$.

There are two immediate implication of weak duality:

1. If both $$\pmb x$$ and $$\pmb y$$ are feasible solutions to the primal and dual and $$\pmb c^\top \pmb x=\pmb b^\top \pmb y$$, then $$\pmb x$$ and $$\pmb y$$ must be optimal solutions
2. If the optimal profit in the primal is $$\infty$$, then the dual must be infeasible. If the optimal cost in the dual is $$-\infty$$, then the primal must be infeasible

The weak duality always holds regardless whether the primal is convex or not.

## Strong Duality

**Theorem 2.** The duality has an optimal solution if and only if the primal does. If both $$\pmb x^*$$ and $$\pmb y^*$$ are optimal solutions to the primal and dual, then $$\pmb c^\top \pmb x^*=\pmb b^\top \pmb y^*$$.

The strong duality only holds when the primal is concave.

To prove strong duality, we first introduce Farkas' Lemma and its variant.

### Farkas' Lemma

**Lemma 1.** Let $$\pmb A\in\mathbb R^{m⨉ n}$$ and $$\pmb b\in\mathbb R^m$$. Then exactly one of the following two assertions is true:

1. There exists an $$\pmb x\in\mathbb R^n$$ such that $$\pmb A\pmb x=\pmb b$$ and $$\pmb x\ge 0$$
2. There exists a $$\pmb y\in\mathbb R^m$$ such that $$\pmb A^\top\pmb y\ge \pmb0$$ and $$\pmb b^\top\pmb y<0$$

**Proof:** If both $$\eqref{eq:1}$$ and $$\eqref{eq:2}$$ hold, then $$0>\pmb b^\top \pmb y=\pmb y^\top \pmb A\pmb x\ge 0$$, which is a contradiction.

We resorts to the following geometric interpretation to show that either $$\eqref{eq:1}$$ or $$\eqref{eq:2}$$ is true.

**Geometric interpretation:** Consider the closed convex cone spanned by the columns of $$\pmb A$$; that is

$$
C(\pmb A)=\{\pmb A\pmb x|\pmb x\ge 0\}
$$

Farkas' lemma equally says that either $$\pmb b$$ in $$C(\pmb A)$$ or there exists a hyperplane $$P=\{\pmb x\vert \pmb y^\top \pmb x=0\}$$ that separates $$\pmb b$$ and $$C(\pmb A)$$. If $$\pmb b$$ is not in $$C(\pmb A)$$, then we can compute $$\pmb y$$ as the vector orthogonal to the hyperplane. The following figure visualizes these two cases

<figure>
  <img src="{{ '/images/math/Farkas-Lemma.jpg' | absolute_url }}" alt="" width="1000">
  <figcaption>The left figure visualize case 1, where b lies in Ax. The right figure visualize case 2, where b is not in C(A). In case 2, we can always find a hyperplane that separates C(A) and b and compute its orthogonal vector y</figcaption>
  <style>
    figure figcaption {
    text-align: center;
    }
  </style>
</figure>

#### A Variant of Farkas' Lemma

**Lemma 2.** Let $$\pmb A\in\mathbb R^{m⨉ n}$$ and $$\pmb b\in\mathbb R^m$$. Then exactly one of the following two assertions is true:

1. There exists an $$\pmb x\in\mathbb R^n$$ such that $$\pmb A\pmb x\le\pmb b$$
2. There exists a $$\pmb y\ge 0$$ such that $$\pmb A^\top\pmb y= \pmb0$$ and $$\pmb b^\top\pmb y=-1$$

**Proof:** By multiplying both sides of $$\pmb A\pmb x\le\pmb b$$ by $$\pmb y^\top$$, it is easy to see that both cases cannot hold simultaneously. 

If case $$\eqref{eq:1}$$ fails to hold, there is no $$\pmb x^+,\pmb x^-\in\mathbb R^n$$ and $$\pmb s\in\mathbb R^m$$ such that $$\begin{bmatrix}\pmb x^+\\\\pmb x^-\\\\pmb s\end{bmatrix}\ge \pmb 0$$

$$
\begin{bmatrix}
\pmb A&-\pmb A&\pmb I_m
\end{bmatrix}
\begin{bmatrix}\pmb x^+\\\\pmb x^-\\\\pmb s\end{bmatrix}=
\pmb b\\\
$$

By Farkas' Lemma, there exists $$\pmb y\in \mathbb R^m$$ such that $$\pmb y\ge \pmb 0$$

$$
\pmb y^\top
\begin{bmatrix}\pmb A& -\pmb A&\pmb I_m\end{bmatrix} 
\ge\pmb 0,\quad
\pmb y^\top\pmb b<\pmb 0
$$

Therefore, we have $$\pmb y^\top \pmb A\ge 0$$, $$-\pmb y^\top\pmb A\ge 0$$, which gives us $$\pmb y^\top\pmb A=0$$. Because $$\pmb b^\top\pmb y< 0$$ and the magnitude of $$\pmb y$$ does not affect the results we can scale $$\pmb y$$ so that $$\pmb b^\top\pmb y=-1$$.

If case $$\eqref{eq:2}$$ fails to hold, there is no $$\pmb y\in \mathbb R^m$$ such that

$$
\begin{bmatrix}
\pmb A& \pmb I_m
\end{bmatrix}^\top \pmb y \ge \pmb 0,\quad \pmb b^\top \pmb y=-1
$$

By Farkas' Lemma, there exists $$\pmb x\in \mathbb R^n, \pmb z\in\mathbb R^m$$ such that $$\begin{bmatrix}\pmb x\\\\pmb z\end{bmatrix}\ge \pmb 0$$

$$
\begin{bmatrix}
\pmb A& \pmb I_m
\end{bmatrix}
\begin{bmatrix}
\pmb x\\\\pmb z
\end{bmatrix}=\pmb b
$$

Therefore, $$\pmb A\pmb x+\pmb z=\pmb b$$ and case $$\eqref{eq:1}$$ holds.

### Proof of Strong Duality

Let

$$
\pmb{\hat A}=\begin{bmatrix}\pmb A\\\-\pmb c^\top\end{bmatrix},\pmb{\hat b}=\begin{bmatrix}\pmb b\\\-(\tau+\epsilon)\end{bmatrix}
$$

where $$\tau=\pmb c^\top\pmb x^*$$ is the optimal value of the primal, $$\epsilon\ge 0$$ is an arbitrary small value. Because $$\tau$$ is the optimal value, there is no feasible $$\pmb x$$ such that $$\pmb c^\top\pmb x=\tau+\epsilon$$. Therefore, there is no $$\pmb x\in\mathbb R^n$$ such that

$$
\begin{bmatrix}\pmb A\\\-\pmb c^\top\end{bmatrix}\pmb x\le \begin{bmatrix}\pmb b\\\-(\tau+\epsilon)\end{bmatrix}
$$

By Lemma 2, there exists $$\pmb{\hat y}=\begin{bmatrix}\pmb y\\\ \alpha\end{bmatrix}\ge\pmb 0$$, such that

$$
\begin{bmatrix}\pmb A^\top&-\pmb c\end{bmatrix}\begin{bmatrix}\pmb y\\\\alpha\end{bmatrix}=\pmb 0,
\quad \begin{bmatrix}\pmb b^\top&-(\tau+\epsilon)\end{bmatrix}\begin{bmatrix}\pmb y\\\\alpha\end{bmatrix}<0\\\
$$

Thus, we have

$$
\pmb A^\top\pmb y=\alpha\pmb c,\quad \pmb b^\top\pmb y<\alpha(\tau+\epsilon)
$$

If $$\alpha=0$$, then the dual is either infeasible or unbounded -- Because of $$\pmb A^\top\pmb y= \pmb 0$$ and $$\pmb b^\top\pmb y<0$$, for any feasible solution to the dual $$\pmb b^\top\pmb y_*$$, we can always find a smaller feasible solution $$\pmb b^\top(\pmb y_*+\pmb y)$$. Therefore, $$\alpha>0$$ and by scaling $$\pmb {\hat y}$$, we may assume that $$\alpha=1$$. So $$\pmb A^\top \pmb y\ge \pmb c$$ and $$\pmb b^\top\pmb y\le \tau+\epsilon$$. By the Weak Dual Theorem, we have $$\tau= \pmb c^\top \pmb x\le\pmb b^\top\pmb y\le\tau+\epsilon$$. Since $$\epsilon$$ is arbitrary, we have $$\pmb c^\top \pmb x=\pmb b^\top\pmb y$$.

## Miscellanea

Here, we prove KKT conditions using Farkas' Lemma

### KKT conditions

**KKT conditions:** Suppose that $$f,g^1,g^2,g^M$$ are differentiable functions from $$\mathbb R^N$$ to $$\mathbb R$$. Let $$\pmb x^*$$ be the point that maximizes $$f$$ subject to $$g^i(\pmb x)\le 0$$ for each $$i$$, and assume the first $$k$$ constraints are active and $$\pmb x^*$$ is regular. Then there exists $$\pmb y\in\mathbb R^k$$ such that $$\pmb y\ge \pmb 0$$ and $$\nabla f(\pmb x^*)=y_1\nabla g^1(\pmb x^*)+...+y_k\nabla g^k(\pmb x^*)$$

**Proof:** Let $$c=\nabla f(\pmb x^*)$$. The directional derivative of $$f$$ in the direction of vector $$\pmb z$$ is $$\pmb z^\top\pmb c$$. Because $$\pmb x^*$$ is the maximum point, every feasible direction $$\pmb z$$ must have $$\pmb z^\top\pmb c < 0$$. Let $$\pmb A\in\mathbb R^{k\times N}$$ be the matrix whose $$i$$th row is $$(\nabla g^i(\pmb x))^\top$$. Then a direction $$\pmb z$$ is feasible if $$\pmb A\pmb z\le 0$$. As $$\pmb z^\top\pmb c<0$$ and $$\pmb A\pmb z\le 0$$ violate Farkas' case $$\eqref{eq:2}$$ , there is a $$\pmb y\ge \pmb 0$$ such that $$\pmb A^\top\pmb y=\pmb c$$.

### Thoughts

In the above proofs, we saw several tricks used in linear programming.

1. We can convert an inequality to equality with recourse to a slack variable. For example, $$\pmb A\pmb x\le\pmb b$$ is equivalent to $$\begin{bmatrix}\pmb A&\pmb I\end{bmatrix}\begin{bmatrix}\pmb x\\\\pmb s\end{bmatrix}=\pmb b$$ for $$\pmb s\ge \pmb0$$. 
2. To get $$\pmb y^\top\pmb A=\pmb0$$, we construct $$\pmb y^\top\begin{bmatrix}\pmb A& -\pmb A\end{bmatrix}$$ and prove it's greater than or equal to $$\pmb 0$$.
3. When we have $$\pmb y\ge 0$$ and $$\pmb A^\top \pmb y\ge 0$$, we can combine them in a single inequality $$\begin{bmatrix}\pmb A^\top\\\\pmb I\end{bmatrix}\pmb y$$
4. To prove $$\pmb c^\top\pmb x=\pmb y^\top\pmb b$$ with known $$\pmb c^\top\pmb x\le\pmb y^\top\pmb b$$, we show $$\pmb y^\top\pmb b\le \pmb c^\top\pmb x+\epsilon$$ for arbitrary $$\epsilon$$.

### References

https://web.stanford.edu/~ashishg/msande111/notes/chapter4.pdf

http://www.ma.rhul.ac.uk/~uvah099/Maths/Farkas.pdf

http://people.csail.mit.edu/moitra/docs/6854lec11.pdf