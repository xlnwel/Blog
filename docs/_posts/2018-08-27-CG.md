---
title: "CG — Conjugate Gradient Method"
excerpt: "In which we talk about the conjugate gradient method in chaos :-)"
categories:
  - Mathematics
tags:
  - Mathematics
---

## Conjugate Gradient Method

In which we want to solve the system of linear equation

$$
{Ax=b}
$$

for the vector $$ x $$, where $$ A $$ is symmetric, positive-definite(i.e. $$ x^TAx>0 $$ for all non-zero vectors $$ x $$ in $$ R^n $$), and real, and $$ b $$ is known as well. we denote the unique solution (which is guaranteed by $$ A $$ being positive definite, later we'll see [how](#unique)) to this system by $$ x_* $$.

### Preliminaries

#### Conjugate

Two vectors $$ u, v $$ are conjugate (with respect to $$ A $$) if 

$$
\langle u,v\rangle_{A}=u^T{Av}=0\tag{1}\label{eq:1}
$$

where the known $$ n\times n $$ matrix $$ A $$ is symmetric, positive-definite.

### A Direct Method

Suppose that

$$
P=\{p_1, \dots, p_n\}
$$

is a set of mutually conjugate vectors (with respect to $$ A $$). Then $$ P $$ forms a basis for $$ \mathcal R^n $$ (proof will be shown later <a name='back1'></a>[here](#basis)), and we may express the solution $$ x_* $$ of $$ Ax=b $$ in this basis

$$
x_{*}=\sum_{i=1}^n\alpha_i p_i
$$

Then we calculate

$$
b=Ax_*=\sum_{i=1}^n\alpha_i A p_i
$$

Left multiply it by $$ p_k^T $$

$$
\begin{align}
\sum_{i=1}^n \alpha_i p_k^T{Ap}_i&=p_k^Tb\\\
\alpha_kp_k^T{Ap}_k&=p_k^Tb\\\
\alpha_k&={\langle p_k^T,b\rangle\over \langle p_k^T, p_k\rangle_A}
\end{align}
$$

by solving every $$ \alpha_k $$ we'll succeed having $$ x_* $$ in basis $$ P $$.

### An Iterative Method

#### Gram-Schmidt Orthogonalization

Given linearly independent vectors, $$ \{x_1, x_2, \dots, x_n\} $$, we build an orthogonal basis $$ \{y_1, y_2,\dots, y_n\} $$ as follows

$$
\begin{align}
y_1 &= x_1\\\
y_{k+1} &= x_{k+1} - \sum_{i=1}^{k}{\langle x_{k+1}, y_i \rangle\over \langle y_i, y_i \rangle}y_i
\end{align}
$$

The $$ A $$-conjugate ($$ A $$-orthogonal) basis is built similarly. Given linearly independent vectors, $$ \{g_1, g_2,\dots, g_n\} $$, it's constructed as below

$$
\begin{align}
d_1 &= -g_1\\\
d_{k+1} &= -g_{k+1} + \sum_{i=1}^{k}{\langle g_{k+1}, d_i \rangle_A\over \langle d_i, d_i \rangle_A}d_i
\end{align}\tag{2}\label{eq:2}
$$


where I intended to negate $$ d_k $$s so as to coordinate with the following discussion.

#### Minimization of The Quadratic Function

Note that $$ x_* $$ is also the unique minimizer of the following quadratic function

$$
f(x)={1\over 2}x^TAx-x^Tb
$$

<a name='unique'></a>To see that $$ x_* $$ is the unique minimizer, we notice that $$ A $$ is positive definite, which suggests $$ x^TAx>0 $$, so $$ f(x) $$ has a unique minimizer. By taking the first derivative, we have

$$
\nabla f(x)=Ax-b
$$

which is $$ 0 $$ at $$ x_* $$. Thus $$ x_* $$ is the unique minimizer of $$ f(x) $$.

Let $$ x=\sum_{i=1}^n \alpha_i d_i $$, where $$ {d_1, \dots, d_n} $$ are $$ A $$-conjugate basis. Then $$ f(x) $$ is expressed as below

$$
f(x)=\sum_{i=1}^n \left({1\over 2}\alpha_i^2\langle d_i, d_i \rangle_A-\alpha_i \langle d_i,b\rangle\right) \tag{3}\label{eq:3}
$$


This suggests that $$ f(x) $$ can be divided into $$ n $$ sub-functions. This is crucial in that it allows us to gradually construct $$ x $$:

$$
\begin{align}
x_0&=0\\\
x_{k+1}&=x_0+\sum_{i=0}^k\alpha_id_i
\end{align}\tag{4}\label{eq:4}
$$

With this at hand, we can verify that when $$x$$ is the minimizer of the quadratic function, the gradient $$ g_{k+1}=\nabla_{x_{k+1}}f(x_{k+1}) $$ is orthogonal to $$ d_1,\dots,d_k $$ (proof is given <a name='back2'></a>[here](#orthogonal)). Because $$ g_k $$ is in the subspace denoted by the $$ A $$-conjugate basis $$ d_1, \dots, d_k $$, it natually follows that $$ g_{k+1} $$ is orthogonal to $$ g_k $$. The same proof applies to all other previously computed gradient. Now we have linearly independent vectors $$ g_1, \dots, g_n $$, the $$ A $$-conjugate basis $$ d_1, \dots, d_n $$ can be constructed from $$ (2) $$. Furthermore, $$ (2) $$ can be simplified to be

$$
\begin{align}
d_{k+1}&=-g_{k+1}+\beta_k d_k\\\
\mathrm {where}\quad\beta_k&={g_{k+1}^T(g_{k+1}-g_k)\over g_k^Tg_k}
\end{align}\tag{5}\label{eq:5}
$$

The detailed derivation is appended at <a name='back3'></a>[the end](#simplify). 

From $$ (3) $$, we can see that minimizing $$ f(x) $$ simply suggests to minimize each $$ \psi_i(\alpha_i) $$, where $$ \psi_i(\alpha_i)={1\over 2}\alpha_i^2\langle d_i, d_i \rangle_A+\alpha_i\langle d_i,b\rangle $$. Thus, we can compute $$ \alpha_i $$ directly by setting the derivative of $$ \psi_i(\alpha_i) $$ to zero.

$$
\begin{align}
\nabla_{\alpha_i}\psi_i(\alpha_i)&=0\\\
\alpha_i\langle d_i, d_i\rangle_A-\langle d_i,b\rangle&=0
\end{align}
$$

then we have

$$
\begin{align}
\alpha_i&={\langle d_i, b\rangle \over \langle d_i,d_i\rangle_A}\\\
&=-{\langle d_i, g_i\rangle \over \langle d_i,d_i\rangle_A}\\\
&={\langle g_i, g_i\rangle \over \langle d_i,d_i\rangle_A}\tag{6}\label{eq:6}
\end{align}
$$

Some explanations:
- We replace $$ b ​$$ with $$ b=Ax_i - g_i ​$$ in the second step and spot that

$$
\begin{align}
d_i^T(Ax_i)^T&=d_i^T\left(A\sum_{j=0}^{i-1}\alpha_jd_j\right)\\\
&=\sum_j\alpha_jd_i^TAd_j\\\
&=0
\end{align}
$$

- The last step is obtained according to $$ d_i=-g_i+\beta_{i-1}d_{i-1} ​$$, which is shown in $$ (5) ​$$, and noticing $$ d_{i-1} ​$$ is orthogonal to $$ g_i ​$$.

We have basically introduced everything needed to iteratively implement conjugate gradient method. Here is still one more trick to add. Notice that the gradient $$ g_{k+1}=Ax_{k+1}-b $$, where $$ x_{k+1} $$ can be iteratively computed, so we expand $$ x_{k+1} $$ and we have

$$
\begin{align}
g_{k+1}&=A(x_k+\alpha_k d_k)-b\\\
&=Ax_k-b+\alpha_k Ad_k\\\
&=g_k+\alpha_kAd_k\tag{7}\label{eq:7}
\end{align}
$$


#### Algorithm

Now summing up $$ (4) $$ to $$ (7) $$, we obtain the following iterative method

$$
\begin{align}
&x_0=0\\\
&g_0=Ax_0-b\\\
&d_0=-g_0\\\
&\mathrm{for}\ k=0, 1, \cdots:\\\
&\quad \alpha_k={\langle g_k, g_k\rangle \over \langle d_k,d_k\rangle_A} & Eq.(6)\\\
&\quad x_{k+1}=x_k+\alpha_k d_k & Eq.(4)\\\
&\quad g_{k+1}=g_k+\alpha_kAd_k & Eq.(7)\\\
&\quad \beta_k={g_{k+1}^T(g_{k+1}-g_k)\over g_k^Tg_k} & Eq.(5)\\\
&\quad d_{k+1}=-g_{k+1}+\beta_kd_k & Eq.(5)
\end{align}
$$


The algorithm can be summarized as simply repeating the following steps

1. Compute $$ x_k $$ in the current conjugate basis $$ d_{1:k-1} $$.
2. Compute $$ g_k $$, the gradient of $$ f(x_k) $$.
3. Calculate the next conjugate vector $$ d_{k} $$.

### Proofs

#### <a name='basis'></a> Proof for Linear Independence of $$ n $$ Mutually Conjugate Vectors

Suppose that

$$
P=\{p_1,\dots,p_n\}
$$

is a set of mutually conjugate vectors w.r.t some positive-definite $$ A $$. We are now going to prove vectors in $$ P $$ are linearly independent.

Assume there exists some $$ p_k $$ such that

$$
\begin{align}
p_k&=\sum_{i=1, i\ne k}^n\alpha_ip_i
\end{align}
$$

sticking it back into $$ (1) $$ we end up with a contradiction

$$
\begin{align}
p_k^TAp_l&=\sum_{i=1, i\ne k}^n\alpha_ip_i^TAp_l\\\
&=\alpha_lp_l^T{Ap}_l\\\
&\ne0
\end{align}
$$

 The last step is derived for the positive-definite property of $$ A $$.

[back to the context](#back1)

#### <a name='orthogonal'></a> Proof for $$ g_{k+1}\bot d_1,\dots,d_k $$

Let $$ A_k=\begin{bmatrix}\alpha_1\\\ \dots\\\ \alpha_k\end{bmatrix} $$ and $$ D_k=\begin{bmatrix}d_1\\\ \dots\\\ d_k\end{bmatrix} $$, now we have 

$$
x_{k+1}=x_0+D_k^TA_k
$$

to find $$ A_k $$ that minimizes $$ f(x_{k+1}) $$, we compute $$ A_k $$ when the gradient is $$ 0 $$. That is,

$$
\begin{align}
\nabla_{A_k} f(x_{k+1})&=0\\\
D_k\nabla_{x_{k+1}}f(x_{k+1})&=0
\end{align}
$$

The above equation suggests 

$$
\begin{align}
d_1\nabla_{x_{k+1}}f(x_{k+1})&=d_1g_{k+1}=0\\\
d_2\nabla_{x_{k+1}}f(x_{k+1})&=d_2g_{k+1}=0\\\
\vdots
\end{align}
$$

so we have

$$
g_{k+1}\bot d_1,\dots,d_k
$$


[back to the context](#back2)

#### <a name='simplify'></a> Simplified Iterative Derivation of $$ d $$

We have stated that

$$
\begin{align}
d_1 &= -g_1\\\
d_{k+1} &= -g_{k+1} + \sum_{i=1}^{k}{\langle g_{k+1}, d_i \rangle_A\over \langle d_i, d_i \rangle_A}d_i
\end{align}\tag{2}\label{eq:2}
$$

now let us take a deeper look at it and see what we will get.

According to $$ (4) $$, we derive

$$
d_i={1\over \alpha_i}(x_{i+1}-x_i)
$$

Left multiplying $$ A $$, we get

$$
Ad_i={1\over \alpha_i}A(x_{i+1}-x_i)={1\over \alpha_i}(g_{i+1}-g_i)
$$

Plugging it back to $$ (2) $$, we obtain

$$
\begin{align}
d_{k+1}&=-g_{k+1}+\sum_{i=1}^k{g_{k+1}^T(g_{i+1}-g_{i})\over d_i^T(g_{i+1}-g_i)}d_i\\\
&=-g_{k+1}+{g_{k+1}^T(g_{k+1}-g_k)\over d_k^T(g_{k+1}-g_k)}d_k&\mathrm{since}\ g_{k+1}\bot g_1, \dots, g_k\\\
&=-g_{k+1}+{g^T_{k+1}(g_{k+1}-g_k)\over -d_k^Tg_k}d_k&\mathrm{since}\ d_k\bot g_{k+1}\\\
&=-g_{k+1}+{g^T_{k+1}(g_{k+1}-g_k)\over g_k^Tg_k}d_k&\mathrm{since}\ d_k=-g_k+\beta_{k-1}d_{k-1}\\\
&=-g_{k+1}+\beta_kd_k&\mathrm{where}\ \beta_k={g^T_{k+1}(g_{k+1}-g_k)\over g_k^Tg_k}
\end{align}
$$

for some mysterious (numerical) reason, we generally don't further simplify the denominator of $$ \beta_k $$

[back to the context](#back3)