---
title: "LSTM"
excerpt: "In which we introduce LSTM, its variants and some implementation specifics"
categories:
  - Deep Learning
tags:
  - Deep Learning
  - Network Architecture
---

## <a name="dir"></a>What's this post about?

This post is for those who have basic understanding about the recurrent neural network, and want to recap some essential idea and implementation details from time to time — such as myself :-). If you want to learn LSTM thoroughly, I strongly recommend [this excellent post](http://colah.github.io/posts/2015-08-Understanding-LSTMs/).

Of course, this post may still worth your time, since it contains more than just LSTM. First I'll make a concise introduction to LSTM and its variants. Then I'll cover some implementation details when using *tensorflow* to implement LSTM. In the end, I'll talk about embedding, a very useful technique in natural language processing. 

​	After reading this post, you might get a sense of

- [Why LSTM](#why) network outperforms the traditional RNN
- The architecture of [LSTM network](#net), and [LSTM unit](#unit)
- [LSTM's variants](#var), especially GRU
- [Embedding](#embedding), why do we use embedding, and how it works
- Some [details](#imp) when using *tensorflow* to implement a LSTM net

At the end of the post, I'll from time to time append [links](#application) to some state-of-the-art applications of RNN for further reference

## <a name="why"></a>Why LSTM?

Traditional RNN fails in two ways: 

1. The old memory fades as new memory involves — the vanishing gradient occurs because of the multiplication happening between timestamps

2. It doesn't distinguish the important from the trivial — there is no selection method in the traditional RNN

LSTM networks escape from the first problem by using the *addition* operation instead of multiplication — the only multiplication for the cell state is the forget gate, which usually approximates to one. And they resort to *gates* to get rid of the second problem

[Elevator back to directory](#dir)

## <a name="net"></a>Architecture of LSTM Network

![LSTM]({{ '/images/rnn/architecture.png' | absolute_url }})

I don't know what to say, this picture has explained everything… Let's jump to the next section. 

Okay, let's just adress some aliases so as to be consistent with the next image:

- Long-term memory (LTM), is also known as the *cell state*
- Short-term memory (STM), is also known as the *hidden state*

LTM, STM, OUTPUT, EVENT are all feature vectors.  

[Elevator back to directory](#dir)

## <a name="unit"></a>LSTM Unit

![lstm unit]({{ '/images/rnn/lstm-unit.png' | absolute_url }})

I wanna go to the next section… All right, let's briefly recap it.

#### Graph Explaination

Start from three gates, i.e. sigmoid layers $$ \sigma $$, which decide how much a term should be retained

- $$ f_t $$, aka. *forget gate*: it decides what in the cell state to retain and what to forget
- $$ i_t $$, aka. *input gate*: it decides how much of the input to let into the cell state
- $$ o_t $$, aka. *output gate*: it decides which part of the cell state to output, and to save as a new hidden state

$$ \tanh $$s just scale the value between $$ -1 $$ and $$ 1 $$

*Multiplications* extract useful information and drop the useless with respect to the gate

The *addition* operation combines all the useful information in the old cell state, hidden state, and input to construct a new cell state. It's the key that LSTM preserves its long-term memory since additions won't cause vanishing gradient problem.

#### Step-by-step Walk Through

1. The *forget gate* decides what should be retained in the *cell state*
2. The first $$ \tanh $$ creates a vector of new *candidate values*, and the *input gate* decides what to let into the *cell state*
3. The *cell state* drops the things filtered by the *forget gate*, and add those *candidate values* filtered by the *input gate*
4. The second $$ \tanh $$ then scale the values in cell state, and let the *output gate* decides what to output

[Elevator back to directory](#dir)

## <a name="var"></a>Variants

### Add Peephole to Gates

![peephole]({{ '/images/rnn/peephole.png' | absolute_url }})

It adds the peephole connection to gates so as to let them look at the cell state

### Combine Forget and Input Gate

![combine gate]({{ '/images/rnn/combine-gate.png' | absolute_url }})

Instead of separately deciding what to forget and what to add, it makes those decision together: add those we're going to forget and forget those we're going to add

### Gated Recurrent Unit (GRU)

![GRU]({{ '/images/rnn/gru.png' | absolute_url }})

This one may worth some time to talk about. It's been growing increasingly popular since it's simpler than LSTM models, and even in some cases, it performs better than LSTM

#### Step-by-step Walk Through

Since $$ h $$ serves as both the output and the cell state, to avoid ambiguity, I don't name it and just $$ h $$ in the following steps. 

1. $$ r_t $$, *reset gate*, gets inputs from both the $$ x_t $$ and the $$ h_{t-1} $$, and decides what in $$ h_{t-1} $$ should be combined with $$ x_t $$ to construct the *candidate values*
2. $$ \tanh $$ scales the *candidate values*, output $$ \hat h_t $$
3. $$ z_t $$, *update gate*, which combines both the forget and input gate in the LSTM, decides what in $$ h_{t-1} $$ to forget and what in $$ \hat h_t $$ to let in
4. Combining both $$ h_{t-1} $$ and $$ \hat h_t $$, both filtered by $$ z_t $$, yields $$ h_t $$  

[Elevator back to directory](#dir)

## <a name="embedding"></a>Embedding

When dealing with words or phrases, we may have thousands of different data. In that case, if we use one-hot encoding to encode those data, we'll end up with vectors with thousands of $$ 0 $$ and only one $$ 1 $$. It causes both memory inefficiency and a waste of computation.

Notice that $$ XW $$, where $$ X $$ is a one-hot-encoded vector whose $$ i $$th element is $$ 1 $$, results in the $$ i $$th row in $$ W $$. In light of that, we could directly use the $$ i $$th row in $$ W $$ to represent $$ X $$, which is exactly what embedding does. In fact, we don’t need to represent $$ X $$ in a one-hot-encoding form any more — it is fine just to be an integer, $$ i $$ in above example. This technique is wildly used in **natural language processing** (NLP) where *words* or *phrases* from the vocabulary are mapped to vectors of real numbers.

[Elevator back to directory](#dir)


## <a name="imp"></a>Tensorflow Implementation Specifics

**Legacy code. May no longer be compatible with the current version of Tensorflow**

#### Input

RNN takes huge advantage of matrix multiplication, which allows the network to train sequences in a batch simultaneously. But this also brings some extra cautions when building batches. 

- *Make sure batches are full*: each batch must contains exactly $$ N \times M $$ pieces of input data, where $$ N $$ is the batch size, i.e., number of sequences, and $$ M $$ is the number of steps. This means we may have to discard the surplus data
- *Make sure sequences between adjacent batches are connected in the original context*: For example, given a text "abcdefgh", to divide it into 2 batches of shape $$ 2 \times 2 $$, the first batches should look like $$ [[a,b], [e,f]] $$ rather than $$ [[a,b], [c,d]] $$. One way to achive this, is to divide the data into $$ N $$ sequences, then each batch extracts $$ M $$ pieces of data from each sequence
- *The input data need be further processed* (maybe not needed in some case). The input to RNN has shape of $$ (N, M, F) $$, where $$ F $$ is the number of hidden units used to represent a piece of data — common strategies contain one-hot encoding and embedding

#### LSTM Cells

- To build multiple LSTM layers using `tf.contrib.rnn.MultiRNNCell`, we need to pass a list of objects in it and those objects should be different.

  ```python
  tf.contrib.rnn.MultiRNNCell([cell] * num_layers)
  ```

  above code is wrong since it results in a list of the same objects. Usually, we need a `for` loop to do the job

- Don't forget to obtain the initial states for the network, which we'll need to set to a new state during the course of training and predicting

##### Python Code

```python
def build_lstm(self, num_units, num_layers, batch_size, keep_prob):
    """ Build LSTM layer

    :param num_units: Number of hidden units
    :param num_layer: Number of LSTM layers
    :param batch_size: Number of sequences per batch
    :param keep_prob: Keep probability
    :return: LSTM cell and initial state
    """
    def cell(num_units, keep_prob):
        """ construct an RNN cell wrapped by a droupout layer"""
        cell = tf.contrib.rnn.BasicLSTMCell(num_units)
        dropout = tf.contrib.rnn.DropoutWrapper(cell, output_keep_prob=keep_prob)
        
        return dropout
    # use for loop to construct different cell objects
    cells = tf.contrib.rnn.MultiRNNCell([cell(num_units, keep_prob) for _ in range(num_layers)])
    # initial cell state for each sequence
    initial_state = cells.zero_state(batch_size, tf.float32)
    
    return cells, initial_state
```

#### Optimization

LSTMs use addition to avoid vanishing gradient problem, but their gradients can still grow without bound. To fix this, we generally clip the gradients above some threshold

##### Python Code

```python
def build_optimizer(loss, learning_rate, grad_clip):
    ''' Build optmizer for training, using gradient clipping. '''
    
    # Optimizer for training, using gradient clipping to control exploding gradients
    tvars = tf.trainable_variables()
    grads, _ = tf.clip_by_global_norm(tf.gradients(loss, tvars), grad_clip)
    train_op = tf.train.AdamOptimizer(learning_rate)
    optimizer = train_op.apply_gradients(zip(grads, tvars))
    
    return optimizer
```

#### Construct RNN Network

Use `tf.nn.dynamic_rnn` instead of calling the RNN cell directly to create an RNN network. The difference is calling the RNN cell creates an unrolled graph for a fixed RNN length. That means if you call the RNN cell with inputs having 200 time steps you are creating a static graph with 200 RNN steps. First, graph creation is slow. Second, you’re unable to pass in longer sequences (> 200) than you’ve originally specified. `tf.nn.dynamic_rnn` solves this. It uses a `tf.while_loop` to dynamically construct the graph when it is executed. That means graph creation is faster and you can feed batches of variable size.

`tf.nn.dynamic_rnn` requires to specify `initial_state` for cell states and outputs an output tensor along with the `final_state`. `initial_state` is the result of `LSTMCell.zero_state` at the beginning of an episode and during an episode, it is reset to the `final_state` ouput by the previous call of `tf.nn.dynamic_rnn`

#### Training & Prediction

- Both training and prediction require to initialize the cell state at the beginning of an *epoch* and reset it at the beginning of each *batch* iteration to a state, which is returned from previous batch iteration.

[Elevator back to directory](#dir)

## <a name="application"></a>Application

This post ["Attention and Augmented Recurrent Neural Networks"](https://distill.pub/2016/augmented-rnns/) illustrates several promising applications of RNN