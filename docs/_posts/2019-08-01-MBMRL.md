---
title: "MB-MRL — Model-Based Meta Reinforcement Learning"
excerpt: "In which we discuss a model-based meta reinforcement learning algorithm that enables the agent to fast adapt to changes of environment."
categories:
  - Reinforcement Learning
tags:
  - Reinforcement Learning
  - Meta-Learning
  - Model-Based Reinforcement Learning
---

## Introduction

Much ink has been spilled on with model-free meta-RL in the previous posts[[1]({{ site.baseurl }}{% post_url 2019-06-27-adaptive-MAML %})], [[2]({{ site.baseurl }}{% post_url 2019-07-27-PEARL %})]. In this post, we present a model-based meta-RL framework that can adapt fast to changes of environment dynamics. Long story short, this method learns a dynamic model that can fast adapt to the environment changes and uses this model to do model predictive control([MPC](https://en.wikipedia.org/wiki/Model_predictive_control#targetText=Model predictive control (MPC) is,oil refineries since the 1980s.)) to take actions. It is worth noting that the adaptive nature of the learned model is especially important not only in meta-learning but also in model-based RL since it alleviates the requirement of a globally accurate model, which plays an important role in model-based RL.

## Preliminaries

We define a distribution of environment $$\mathcal E$$ as $$\rho(\mathcal E)$$. We forgo the episodic framework, where tasks are pre-defined to be different rewards or environments, and tasks exists at the trajectory level only. Instead, we consider each timestep to potentially be a new "tasks", where any detail or setting could have changed at any timestep. For example, a real legged millirobot unexpectedly loses a leg when moving forward as the following figure shows

<figure>
  <img src="{{ '/images/meta/robot losing leg.png' | absolute_url }}" alt="" width="1000">
  <figcaption></figcaption>
  <style>
    figure figcaption {
    text-align: center;
    }
  </style>
</figure>

We assume that the environment is locally consistent, in that every segment of length $$j-i$$ has the same environment. Even though this assumption is not always correct, it allows us to learn to adapt from data without knowing when the environment has changed. Due to the fast nature of adaptation (less than a second), this assumption is seldom violated.

## Problem Setting

We formulate the adaptation problem as optimizing for the parameters of learning procedure $$\theta,\psi$$ as follows:

$$
\min_{\theta,\psi}\mathbb E_{\mathcal E\sim\rho(\mathcal E)}\big[\mathbb E_{\tau_{\mathcal E}(t-M,t+K)\sim\mathcal D}[\mathcal L(\tau_{\mathcal E}(t,t+K), \theta'_{\mathcal E})]\big]\tag{1}\label{eq:1}\\\
s.t. \theta'_{\mathcal E}=u_\psi(\tau_{\mathcal E}(t-M, t-1), \theta)
$$

where $$\tau_{\mathcal E}(t-M,t+K)$$ corresponds to trajectory segments sampled from previous experiences, $$u_\psi$$ is the adaptation process. $$\mathcal L$$ denotes the dynamics loss function, which, as we discussed in the [previous post]({{ site.baseurl }}{% post_url 2018-12-07-MBMF %}), is the mean squared error of changes in state $$\Delta s$$(see the official implementation [here](https://github.com/iclavera/learning_to_adapt/blob/bd7d99ba402521c96631e7d09714128f549db0f1/learning_to_adapt/dynamics/meta_mlp_dynamics.py#L133)):

$$
\mathcal L(\tau_{\mathcal E}(t,t+K),\theta'_{\mathcal E})={1\over K}\sum_{k=t}^{t+K-1}\big((s_{k+1} - s_{k})-f_{\theta}(s_{k},a_{k})\big)^2
$$

Intuitively, by optimizing Eq.(1), we expect the agent can do well in the next $$K$$ steps after the agent adapts the model according to the past $$M$$ transitions. 

Also, notice that in Eq.(1), we put all data in a single dataset $$\mathcal D$$ instead of maintaining one dataset for each task since we want to do fast adaptation instead of task transfer here.

## Benefit of Combining Meta-Learning with Model-Based RL

Learning to adapt a model alleviates a central challenge of model-based reinforcement learning: the problem of acquiring a global model that is accurate throughout the entire state space. Furthermore, even if it were practical to train a globally accurate dynamics model, the dynamics inherently change as a function of uncontrollable and often unobservable environmental factors. If we have a model that can adapt online, it need not be perfect everywhere a priori. This property has previously been exploited by adaptive control methods (Åström and Wittenmark, 2013; Sastry and Isidori, 1989; Pastor et al., 2011; Meier et al., 2016); but, scaling such methods to complex tasks and nonlinear systems is exceptionally difﬁcult. Even when working with deep neural networks, which have been used to model complex nonlinear systems (Kurutach et al., 2018), it is exceptionally difﬁcult to enable adaptation, since such models typically require large amounts of data and many gradient steps to learn effectively. By speciﬁcally training a neural network model to require only a small amount of experience to adapt, we can enable effective online adaptation in complex environments while putting less pressure on needing a perfect global model. (excepted from the original paper)

## Model-Based Meta-Reinforcement Learning

Nagabandi&Clavera et al. introduce two approaches to solve Objective.$$\eqref{eq:1}$$. One is based on gradient-based meta-learning, the other is based on recurrent models. Both share the same framework and only differ in network architecture and optimization procedure. In fact, since they orthogonally emphasize different parts of the framework, they may be combined to form a more powerful method in the end.

### Gradient-Based Adaptive Learner

Gradient-Based Adaptive Learner(GrBAL) uses a gradient-based meta-learning to perform online adaptation; the update rule is prescribed by gradient descent:

$$
\theta'_{\mathcal E}=u_\psi(\tau_{\mathcal E}(t-M, t-1), \theta)=\theta_{\mathcal E}+\psi\nabla_{\theta}{1\over M}\sum_{m=t-M}^{t-1}\big((s_{m} - s_{m-1})-f_{\theta}(s_{m-1},a_{m-1})\big)^2
$$

Here $$\psi$$ is the step sizes at adaptation time.

### Recurrence-Based Adaptive Learner

Recurrence-Based Adaptive Learner(ReBAL) utilizes a recurrent model(or attention model), which learns its own update rule through its internal structure. In this case, $$\psi$$ and $$u_\psi$$ correspond to the weights of the model.

## Algorithm

<figure>
  <img src="{{ '/images/meta/mbmrl.png' | absolute_url }}" alt="" width="1000">
  <figcaption>Pseudocode</figcaption>
  <style>
    figure figcaption {
    text-align: center;
    }
  </style>
</figure>

If you've already familiar with model-agnostic meta-learning([MAML]({{ site.baseurl }}{% post_url 2019-06-07-MAML %})) and model predictive control(MPC), there is nothing new here. We learn an adaptive dynamics model via Algorithm 1, and then at each time step, the agent first adapts the model and then perform MPC to take actions as shown in Algorithm 2.

## Experimental Results

See the following video for experimental results

<iframe width="650" height="415" src="https://www.youtube.com/embed/ejG2nzCNdZ8" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
## References

Anusha Nagabandi, Ignasi Clavera, Simin Liu, Ronald S. Fearing, Pieter Abbeel, Sergey Levine, & Chelsea Finn, “Learning to Adapt in Dynamic, Real-World Environments through Meta-Reinforcement Learning,” *ICLR*, pp. 1–17, 2019.

C. Finn, P. Abbeel, and S. Levine, “Model-agnostic meta-learning for fast adaptation of deep networks,” *34th Int. Conf. Mach. Learn. ICML 2017*, vol. 3, pp. 1856–1868, 2017.

Y. Duan, J. Schulman, X. Chen, P. L. Bartlett, I. Sutskever, and P. Abbeel, “RL$$^2$$: Fast Reinforcement Learning via Slow Reinforcement Learning,” *ICLR*, pp. 1–14, 2017.