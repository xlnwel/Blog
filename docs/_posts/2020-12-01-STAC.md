---
title: "Self-Tuning Reinforcement Learning"
excerpt: "In which we present self-tuning reinforcement learning algorithm."
categories:
  - Reinforcement Learning
tags:
  - Reinforcement Learning
  - Model-Free Reinforcement Learning
  - Meta-Gradient Reinforcement-Learning
---

## Introduction

Reinforcement learning algorithms are sensitive to the choice of hyperparameters, typically requires significantly effort to identify hyperparameters that perform well on a new domain. Many works have been done to ease of hyperparameter tuning. For example, [IMPALA]({{ site.baseurl }}{% post_url 2019-11-14-IMPALA %}), [FTW]({{ site.baseurl }}{% post_url 2020-03-27-FTW %}) and [AlphaStar]({{ site.baseurl }}{% post_url 2021-01-14-AlphaStar %}) resort to population-based training to evolve hyperparameters from a group of agents training in parallel, showing great success of evolving hyperparameters during training. However, PBT usually requires a significantly large amount of computational resources to training a family of agents. In this post, we discuss Self-Tuning Actor Critic, a meta-gradient algorithm self-tuning all the differentiable hyperparameters of an actor-critic loss function.

## Preliminaries

As Self-Tuning Actor Critic(STAC) builds upon [IMPALA]({{ site.baseurl }}{% post_url 2019-11-14-IMPALA %}), we briefly cover the loss functions used in IMPALA first

$$
\begin{align}
\mathcal L(\theta)=&g_V\mathcal L_V(\theta)+g_p\mathcal L_\pi(\theta)+g_e\mathcal L_{\mathcal H}(\theta)\\\
\mathcal L_V(\theta)=&\mathbb E_\mu[(v(x_t)-V_\theta(x_t))^2]\\\
\mathcal L_\pi(\theta)=&-\mathbb E_{(x_t,a_t,x_{t+1})\sim\mu}[\rho_t(r_t+\gamma v(x_{t+1})-V(x_t))\log\pi_\theta(a_t|x_t)]\\\
\mathcal L_{\mathcal H}(\theta)=&-\mathcal H(\pi_\theta)
\end{align}\tag 1
$$

where the target value $$v(x_t)$$ is defined by

$$
\begin{align}
v(x_t) :=& V(x_t)+\sum_{k=t}^{t+n-1}\gamma^{k-t}\left(\prod_{i=t}^{k-1}c_i\right)\delta_kV\\\
\delta_kV:=&\rho_k(r_k+\gamma V(x_{k+1})-V(x_k))\\\
c_{i}:=&\lambda \min\left(\bar c, {\pi(a_i|x_i)\over \mu(a_i|x_i)}\right)\\\
\rho_k:=&\min\left(\bar\rho, {\pi(a_k|x_k)\over \mu(a_k|x_k)}\right)
\end{align}
$$

In STAC, we divide the hyperparameters into two groups: tunable hyperparameters and untunable hyperparameters. The tunable hyperparameters (aka *meta-parameters*) is a subset of differentiable hyperparameters -- in case of STAC, it's $$\eta=\{\gamma,\lambda,g_V,g_p,g_e\}$$. As we'll see in the next section, these meta-parameters are differentiable from an outer meta-objective through inner gradients. 

## Self-Tuning Actor Critic

STAC self-tunes all the meta-parameters following the metagradient framework, which involves an inner loop and an outer loop. In the inner loop, STAC optimizes Equation $$\eqref{eq:1}$$ by taking one step gradient descent w.r.t. $$\mathcal L$$ parameterized by the meta-parameter $$\eta$$ as in IMPALA: $$\tilde\theta(\eta)=\theta-\alpha\nabla_\theta\mathcal L(\theta,\eta)$$. In the outer loop, we cross-validate the new parameters on a subsequent, independent sample--In practice, we use the same sample in both loop for efficient learning--utilizing a differentiable meta-objective $$\mathcal L_{outer}(\tilde\theta(\eta))$$. Specifically, the meta-objective adopted by STAC is

$$
\mathcal L_{outer}(\tilde\theta(\eta))=g_V^{outer}\mathcal L_V(\theta)+g_p^{outer}\mathcal L_\pi(\theta)+g_e^{outer}\mathcal L_{\mathcal H}(\theta)+g_{kl}^{outer}D_{KL}(\pi_{\tilde\theta(\eta)},\pi_\theta)
$$

where $$(\gamma^{outer},\lambda^{outer},g_V^{outer}, g_p^{outer},g_e^{outer}, g_{kl}^{outer})$$ are hyperparameters. For Atari games, they are $$(0.995, 1, 0.25, 1, 0.01, 1)$$. [Zahavy et al. 2020](#ref1) does not explain the motivation of the KL loss, but they do find that with $$g_{kl}^{outer}=1$$ improve the performance. As it regularizes the updated policy against its previous version, one can regard it as a regularization technique imposed by meta-gradient. That is, we want learn meta-gradient so that inner gradient step does not make a big change to our policy.

The gradient of $$\eta$$ is computed by differentiating $$\mathcal L_{outer}(\tilde\theta(\eta))$$ through $$\nabla_\theta\mathcal L(\theta,\eta)$$. That is,

$$
\begin{align}
\nabla_{\eta}\mathcal L_{outer}(\tilde\theta(\eta))=&\nabla_{\tilde\theta(\eta)}\mathcal L_{outer}({\tilde\theta(\eta)})\nabla_\eta{\tilde\theta(\eta)}\\\
=&-\alpha\nabla_{\tilde\theta(\eta)}\mathcal L_{outer}({\tilde\theta(\eta)})\nabla_\eta\nabla_\theta\mathcal L(\theta,\eta)
\end{align}
$$

To ensure that all the meta-parameters are bounded, we apply sigmoid on all of them. We also multiply the loss coefficient $$(g_V,g_p,g_e)$$ by the respective coefficient in the outer loss to guarantee that they are initialized from the same values. For example $$\gamma=\sigma(\gamma)$$, $$g_V=g_V^{outer}\sigma(g_V)$$. We initialize all the meta-parameters to $$\eta^{init}=4.6$$ such that $$\sigma(\eta^{eta})=0.99$$. This guarantees that the inner loss is initialized to be (almost) the same as the outer loss.

### Leaky V-trace

As we've discussed in the previous posts([[1]({{ site.baseurl }}{% post_url 2019-11-14-IMPALA %})], [[2]({{ site.baseurl }}{% post_url 2020-11-14-V-Trace %})]), the fixed policy of the V-trace operator is controlled by the hyperparameter $$\bar\rho$$

$$
\pi_{\bar\rho}(a|x)={\min(\bar\rho\mu(a|x),\pi(a|x))\over{\sum_{b\in A}\min(\bar\rho\mu(b|x),\pi(b|x))}}
$$

The truncation level $$\bar c$$ controls the speed of convergence by trading off variance reduction for convergence rate. Though importance weight clipping effectively reduces the variance, it weakens the effect of later TD errors and worsens the contraction rate. Noticing that, [Zahavy et al. 2020](#ref1) propose *leaky V-trace* that interpolates between the truncated importance sampling and canonical importance sampling. Leaky V-trace use the same target value as V-trace except that

$$
\begin{align}
c_{i}:=&\lambda \big(\min\alpha_c(\bar c, \text{IS}_t)+(1-\alpha_c)\text{IS}_t\big)\\\
\rho_k:=&\min\alpha_\rho(\bar\rho, \text{IS}_t)+(1-\alpha_\rho)\text{IS}_t\\\
where\quad \text{IS}_t=&{\pi(a_i|x_i)\over \mu(a_i|x_i)}
\end{align}
$$

Where $$\alpha_c$$ and $$\alpha_\rho$$ are introduced to allow the importance weights to "leak back" creating the opposite effect to clipping 

Theorem 1 below shows that Leaky V-trace converges to $$V^{\pi_{\bar\rho},\alpha_\rho}$$

**Theorem 1.** *Assume that there exists $$\beta\in(0,1]$$ such that $$\mathbb E_\mu\rho_t\ge\beta$$. Then the operator $$\mathcal R$$ has a unique fixed point $$V^{\pi_{\bar\rho,\alpha_\rho}}$$, which is the value function of the policy $$\pi_{\bar\rho,\alpha_\rho}$$ defined by*

$$
\pi_{\bar\rho,\alpha_\rho}={\alpha_\rho\min(\bar\rho\mu(a|x),\pi(a|x))+(1-\alpha_\rho)\pi(a|x)\over \alpha_\rho\sum_b\min(\bar\rho\mu(b|x),\pi(b|x))+(1-\alpha_\rho)}
$$

*Furthermore, $$\mathcal R$$ is an $$\eta$$-contraction mapping in sup-norm with*

$$
\eta:=\gamma^{-1}-(\gamma^{-1}-1)\mathbb E_\mu\left[\sum_{k\ge t}\gamma^{k-t}\left(\prod_{i=t}^{k-2}c_i\right)\rho_{k-1}\right]\le 1-(1-\gamma)\beta< 1
$$

The proof follows the proof of V-trace with small adaptations for the leaky V-trace coefficient.

Theorem 1 requires $$\alpha_\rho\ge \alpha_c$$, and STAC parameterizes them with a single parameter $$\alpha=\alpha_\rho=\alpha_c$$ and includes it in meta-parameters -- $$\alpha$$ is initialized to $$1$$ and the outer loss is fixed to be V-trace, i.e. $$\alpha^{outer}=1$$. 

## STAC with Auxiliary Tasks

[Zahavy et al. 2020](#ref1) further introduce an agent that extends STAC with auxiliary policy and value heads. The motivation is to utilize meta-gradient to learn different hyperparameters for different policy value heads. For example, auxiliary objectives with different discount factors allow STACX to reason about multiple horizons. In practice, however, these heads serve more like auxiliary tasks as none of them are actually used to sample trajectories; sampling trajectories using auxiliary policies -- even with some ensemble techniques -- compromises the learning of the main policy. This is most likely caused by the choice of the V-trace loss as way off-policy data causes the V-trace operator to converge to value function of policy different from the optimal one.

On account of the introduction of policy and value heads, the meta-parameters for STACX now becomes $$\eta=\{\gamma^i,\lambda^i,g_V^i,g_p^i,g_e^i,\alpha^i\}_{i=1}^3$$. On the other hand, the outer loss does not change; it is still defined only w.r.t. the main heads.

## Universal Value Function Approximation for Meta-Parameters

[Xu et al. 2018](#ref2) in Section 1.4 point out that the target function $$v(x_t)$$ is non-stationary, adapting along with the meta-parameters throughout the training process. As a result, there is a danger that the value function $$v_\theta$$ becomes inaccurate, since it may be approximating old returns. For example, when $$\gamma$$ changes from $$0$$ to $$1$$, the value function learned for $$\gamma=0$$ does no longer provide a valid approximation. 

To deal with non-stationarity in the value function and policy, we turns to universal value function approximation, where we provide the meta-parameter $$\eta$$ as an additional input the condition the value function and policy, as follows:

$$
\begin{align}
V_\theta^\eta(s)=&V_\theta([s;e_\eta])\\\
\pi_\theta^\eta(s)=&\pi_\theta([s;e_\eta])\\\
e_\eta=&W_\eta\eta
\end{align}
$$

Where $$e_\eta$$ is the embedding of $$\eta$$, $$[s;e_\eta]$$ denotes concatenation of vectors $$s$$ and $$e_\eta$$. $$W_\eta$$ is the learnable embedding matrix.

## Experimental Results

### Effect of self-tuning and auxiliary tasks

<figure>
  <img src="{{ '/images/meta-gradient/STAC-Figure2.png' | absolute_url }}" alt="" width="1000">
  <figcaption>The inferior performance of {ùõæ} to Xu et al. is mainly caused by not using embedding</figcaption>
  <style>
    figure figcaption {
    text-align: center;
    }
  </style>
</figure>
Figure 2 shows that both self-tuning and auxiliary tasks improve the performance. Furthermore, Figure. 2(b) shows that leaky V-trace performs better than V-trace. Noticeably, auxiliary tasks does not bring much performance gain when self-tuning is completely off. 

<figure>
  <img src="{{ '/images/meta-gradient/STAC-Table4.png' | absolute_url }}" alt="" width="1000">
  <figcaption>Run on a single machine with 56 CPUs and 8TPUs. Environments are vectorized in C++. The action thread uses 2 TPUs while the learner thread uses 6 TPUs.</figcaption>
  <style>
    figure figcaption {
    text-align: center;
    }
  </style>
</figure>

Table 4 shows that self-tuning does not incur much overheads while auxiliary tasks increases the run time by a noticeable margin. Another experiments with multiple CPU workers and a single GPU learner tells a different story. In that case, STAC requires about $$25\%$$ more time, while the extra run time from the auxiliary tasks is negligible    

## Adaptivity of meta-parameters

[Zahavy et al.](#ref1) further monitor the evolution of meta-parameters during training. We summarize several interesting observations below

1. The meta-parameters of the auxiliary heads are self-tuned to have relatively similar values but different than those of the main head. For example, the main head discount factor converges to $$0.995$$. In contrast, the auxiliary heads' discount factors often change during training and get to lower values.
2. The leaky V-trace parameter $$\alpha$$ is close to $$1$$ at the beginning, but may self-tune near the end of training.
3. When we let $$\alpha_\rho$$ and $$\alpha_c$$ tune separately, STACX self-tunes $$\alpha_\rho\ge\alpha_c$$ most of the time.

Effect of STACX, does they learn different discount factors?

## References

<a name='ref1'></a>Zahavy, Tom, Zhongwen Xu, Vivek Veeriah, Matteo Hessel, and Junhyuk Oh. 2020. ‚ÄúA Self-Tuning Actor-Critic Algorithm,‚Äù no. 1: 1‚Äì34.

<a name='ref2'></a>Xu, Zhongwen, Hado Van Hasselt, and David Silver. 2018. ‚ÄúMeta-Gradient Reinforcement Learning.‚Äù *Advances in Neural Information Processing Systems* 2018-December: 2396‚Äì2407.