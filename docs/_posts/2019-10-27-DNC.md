---
title: "DNC — Differentiable Neural Computer"
excerpt: "In which we discuss a network architecture named Differentiable Neural Computer."
categories:
  - Deep Learning
tags:
  - Deep Learning
  - Network Architecture
---

## Introduction

In the [previous post]({{ site.baseurl }}{% post_url 2019-10-21-NTM %}), we discussed Neural Turing Machines(NTMs), which introduced an external memory to maintain information for later retrieval. In this post, we further discuss a more sophisticated method, namely Differential Neural Computer(DNC). The DNC builds on the same idea as the NTMs — both aim to combine the advantage of neural and computational processing by providing a neural network with read-write access to external memory. On the other hand, it improves NTMs with a more complicated and flexible interaction mechanism, which makes it potentially more powerful than NTMs.

## Overview of Differential Neural Computer

<figure>
  <img src="{{ '/images/network/DNC-architecture.png' | absolute_url }}" alt="" width="1920">
  <figcaption></figcaption>
  <style>
    figure figcaption {
    text-align: center;
    }
  </style>
</figure>

The above figure from the original paper demonstrates the overall architecture of a DNC. The following figure further details its internal structure.

<figure>
  <img src="{{ '/images/network/DNC/DNC.001.png' | absolute_url }}" alt="" width="1920">
  <figcaption>The internal structure of a DNC. Symbols in purple squares are from the interface vector; those in orange squares represent the previous states of DNC; those in red squares denote the current states. Operations are defined in the blue rectangles </figcaption>
  <style>
    figure figcaption {
    text-align: center;
    }
  </style>
</figure>

We first provide a brief overview of each part in the DNC cell:

- The *controller*(generally an LSTM cell) receives inputs from the environment together with previous read vectors and emits an output vector and an interface vector, the latter of which is responsible for the interaction with the write and read module.
- The write module manipulates the memory matrix by integrating the content-based look-up and allocation mechanism. The former generates a weighting by measuring the similarity between the input write key and memory entries, while the latter generates weighting based on the usages of each memory location.
- The read module weights three types of memories: content-based memory instructed by the read keys, and temporal memory in the forward and backward written order. We address the first type of memory using the content-based look-up and the rest using a temporal linking mechanism, which leverages a form of a learnable adjacent matrix to track the sequential order.

## Controller Network

We summarize the controller into three steps

1. At every time-step $$t$$ the controller network receives input vector $$\pmb x_t\in\mathbb R^X$$ from the environment(or a dataset) and a set of $$R$$ read vectors $$\pmb r_{t-1}^1, \dots, \pmb r_{t-1}^R$$ from the memory matrix $$M_{t-1}\in\mathbb R^{N\times W}$$ at the previous time-step, via the read heads. 

2. It then concatenates the read and input vectors to obtain a single controller input vector $$\mathcal X_t=[\pmb x_t;\pmb r_{t-1}^1;\dots,\pmb r_{t-1}^R]$$ and pass it through a deep LSTM architecture:

    
$$
    \begin{align}
    \pmb i_t^l&=\sigma(W_i^l[\mathcal X_t;\pmb h_{t-1}^l;\pmb h_{t}^{l-1}]+\pmb b_i^l)\\\
    \pmb f_t^l&=\sigma(W_f^l[\mathcal X_t;\pmb h_{t-1}^l;\pmb h_{t}^{l-1}]+\pmb b_f^l)\\\
    \pmb s_t^l&=\pmb f_t^ls_{t-1}^l+\pmb i_t^l\tanh(W_s^l[\mathcal X_t;\pmb h_{t-1}^l;\pmb h_{t}^{l-1}]+\pmb b_s^l)\\\
    \pmb o_t^l&=\sigma(W_o^l[\mathcal X_t;\pmb h_{t-1}^l;\pmb h_{t}^{l-1}]+\pmb b_o^l)\\\
    \pmb h_t^l&=\pmb o_t^l\tanh(\pmb s_t^l)
    \end{align}
    $$


    where $$l$$ is the layer index, $$\sigma(x)$$ is the logistic sigmoid function, $$\pmb h_t^l,\pmb i_t^l,\pmb f_t^l,\pmb s_t^l$$ and $$\pmb o_t^l$$ are the hidden, input gate, forget gate, (cell)state and output gate activation vectors, respectively, of layer $$l$$ at time $$t$$; $$\pmb h_t^0=0$$ for all $$t$$; $$\pmb h_0^l=\pmb s_0^l=0$$ for all $$l$$. These are just components of a regular LSTM stack; there is nothing new here. 

3. The controller emits an output vector $$\pmb v_t$$, defined as
    
$$
    \begin{align}
    \pmb v_t&=W_y[\pmb h_t^1;\dots;\pmb h_t^L]
    \end{align}
    $$

    The output vector $$\pmb v_t$$ is then combined with the current read vectors to produce the final output---action distribution for reinforcement learning, or predictive distribution for supervised learning
    
$$
    \begin{align}
    \pmb y_t&=\pmb v_t+W_r[\pmb r_t^1;\dots;\pmb r_t^R]\\\
    &=W[\pmb h_t^1;\dots,\pmb h_t^L;\pmb r_t^1;\dots;\pmb r_t^R]
    \end{align}
    $$

    where, in the second step, we combine $$W_y$$ and $$W_r$$ into a single layer $$W$$. This arrangement allows the DNC to condition its output dimensions on memory that has just been read.
    
    The controller also produces an interface vector $$\pmb \xi_t\in\mathbb R^{(W\times R)+3W+5R+3}$$, defined as
    
$$
    \pmb \xi_t=W_\xi[\pmb h_t^1;\dots;\pmb h_t^L]
    $$

    The interface vector encodes its interaction with memory at the current time step, similar to NTMs discussed in the previous post, but using different mechanisms to build read and write weightings.

## Addressing

### Interface Parameters

Before we describe how the interface vector $$\pmb \xi_t$$ interacts with the external memory of shape $$N\times W$$, we subdivide it as follows:

$$
\pmb \xi_t=\left[\pmb k_t^{r,1};\dots;\pmb k_t^{r,R};
\hat\beta_t^{r,1};\dots\hat\beta_t^{r,R};
\pmb k_t^w;\hat\beta_t^w;
\hat{\pmb e}_t;\pmb v_t;
\hat f_t^1;\dots;\hat f_t^R;
\hat g_t^a;\hat g_t^w;
\hat{\pmb\pi}_t^{1};\dots;\hat{\pmb\pi}_t^R
\right]
$$

The components with $$\hat\cdot$$ are then processed with various functions to ensure that they lie in the correct domain. After that we have the following set of scalars and vectors:

- $$R$$ read keys $$\{\pmb k_t^{r,i}\in\mathbb R^W;1\le i\le R\}$$;
- $$R$$ read strengths $$\{\beta_t^{r,i}=\mathrm{oneplus}(\hat\beta_t^{r,i})\in[1,\infty);1\le i\le R\}$$
- the write key $$\pmb k_t^w\in\mathbb R^W$$;
- the write strength $$\beta_t^w=\mathrm{oneplus}(\hat\beta_t^w)\in[1,\infty)$$;
- the erase vector $$\pmb e_t=\sigma(\hat{\pmb e_t})\in[0,1]^W$$;
- the write vector $$\pmb v_t\in\mathbb R^W$$;
- $$R$$ free gates $$\{f_t^i=\sigma(\hat f_t^i)\in[0,1];1\le i\le R\}$$;
- the allocation gate $$g_t^a=\sigma(\hat g_t^a)\in[0,1]$$;
- the write gate $$g_t^w=\sigma(\hat g_t^w)\in[0, 1]$$; and
- $$R$$ read modes $$\{\pmb \pi_t^i=\mathrm{softmax}(\hat{\pmb\pi}_t^i)\in\mathcal S_3;1\le i\le R\}$$. 

where the oneplus function and unit simplex $$\mathcal S_N$$ are defined as follows:

$$
\mathrm{oneplus}(x)=1+\log(1+e^x)\\\
\mathcal S_N=\left\{\pmb\alpha\in\mathbb R^N:\pmb\alpha_i\in[0,1],\sum_{i=1}^N\pmb\alpha_i=1\right\}
$$

we further introduce a weighting space over $$N$$ locations, which only differs from unit simplex $$\mathcal S_N$$ in that $$\pmb\alpha$$ does not necessarily sum to one.

$$
\Delta_N=\left\{\pmb\alpha\in\mathbb R^N:\pmb\alpha_i\in[0,1],\sum_{i=1}^N\pmb \alpha_i\le 1\right\}
$$


Before we carry on, let's do some math to ensure that $$\pmb \xi_t$$ is of length $$(W\times R)+3W+5R+3$$: All $$\pmb k^{r,i}_t$$ together consumes $$W\times R$$ dimensions; $$\pmb k_t^w,\pmb e_t,\pmb v_t$$ all have length $$W$$, which uses $$3W$$ dimensions in total; there are $$R$$ different $$\hat \beta^{r,i}_t$$, $$R$$ different $$\hat f_t^i$$, and $$3R$$ read modes $$\hat{\pmb\pi}_t^i$$, together expending $$5R$$ dimensions ; $$\beta_t^w, g_t^a,g_t^w$$ are all scalars, therefore one for each.

Also be aware that when we have more than one write heads, we will have $$\pmb \pi_t^i=\in\mathcal S_{2N_w+1}$$, where $$N_w$$ is the number of write heads. This is because we have to keep track of the forward and backward positions for each write head.

### A Quick View of Reading and Writing 

We delay the computation of reading and writing weightings $$\pmb w_t^{r,1},\dots,\pmb w_t^{r,R},\pmb w_t^w$$ to the next sub-section. For now, we assume these weightings have been computed. The read and write vectors are computed in the same way as they are in NTMs


$$
\begin{align}
\pmb r_t^i&=\pmb M_t^{\top}\pmb w_t^{r,i}\\\
\pmb M_t&=\pmb M_{t-1}\circ(\pmb E-\pmb w_t^w\pmb e_t^{\top})+\pmb w_t^w\pmb v_t^{\top}
\end{align}
$$

where $$\circ$$ denotes element-wise multiplication and $$\pmb E$$ is an $$N\times W$$ matrix of ones. Noticeably, the weightings used above do not necessarily sum to one.

### Memory Addressing

As we stated before, the system uses a combination of *content-based addressing* and *dynamic memory allocation* to determine where to *write* in memory, and a combination of *content-based addressing* and *temporal memory linkage* to determine where to *read*. These mechanisms, all of which are parameterized by the interface vectors defined in section 'Interface Parameters', are described below.

#### Content-based addressing

As NTMs, read and write heads use content-based addressing for associative recall and modifying an existing vector in memory, respectively. Importantly, this enables a key that only partially matches the content of a memory location still to be used to attend strongly to that location. Accordingly, it allows a form of pattern completion whereby the value recovered by reading the memory location includes additional information that is not present in the key.

We compute the content weighting through a softmax function with an amplified cosine similarity

$$
\mathcal C(\pmb M,\pmb k,\beta)[i]={\exp(\beta D(\pmb k,\pmb M[i,\cdot]))\over\sum_j\exp(\beta D(\pmb k,\pmb M[j,\cdot]))}\\\
D[\pmb u,\pmb v]={\pmb u\cdot\pmb v\over |\pmb u||\pmb v|}
$$

where $$\pmb k$$ and $$\beta$$ are the read/write key and read/write strength, respectively. We could vectorize this computation as

$$
\mathcal C(\pmb M,\pmb k,\beta)=\text{softmax}(\beta\hat{\pmb M}\hat{\pmb k})
$$

where $$\hat {\pmb M}$$ is $$\pmb M$$ with $$\ell^2$$-normalized rows and $$\hat{\pmb k}$$ is $$\ell^2$$-normalized $$\pmb k$$. Or alternatively, if we take into account the number of heads, i.e. $$\pmb k$$ is of shape $$[\text{batch size, num of write heads, word size}]$$, we will have 

$$
\mathcal C(\pmb M,\pmb k,\beta)=\text{softmax}(\beta\hat{\pmb k}\hat{\pmb M}^\top)
$$

where $$\hat {\pmb k}$$ is $$\pmb k$$ with $$\ell^2$$-normalized last dimension.

#### Dynamic Memory Allocation

To allow the controller to free and allocate memory as needed, Graves&Wayne et al. developed a differentiable analog of the 'free list' memory allocation scheme, whereby a list of available memory locations is maintained by adding to and removing addressing from a linked list. 

1. The controller emits a set of the free gates $$f_t^i$$, one per read head, that determine whether the most recently read locations can be freed.

2. We compute from the free gates the memory retention vector $$\pmb \psi_t\in[0,1]^N$$, which represents how much each location will not be freed by the free gates:
   
$$
   \pmb\psi_t=\prod_{i=1}^R(\pmb 1-f_t^i\pmb w_{t-1}^{r,i})
   $$

where the multiplication indicates the intersection of retained probabilities.
   
3. The memory usage vector can then be defined as 
   
$$
   \pmb u_t=(\pmb u_{t-1}+\pmb w_{t-1}^w-\pmb u_{t-1}\circ\pmb w_{t-1}^w)\circ \pmb\psi_t
   $$

   with $$\pmb u_0=0$$ and $$\pmb u_t\in[0,1]^N$$. Intuitively, locations are used if they have been retained by the free gates ($$\pmb\psi_t[i]\approx \pmb 1$$), and either were already in use($$\pmb u_{t-1}\approx \pmb 1$$) or have have just been written to($$\pmb w_{t-1}^w\approx \pmb 1$$). The usage can only be subsequently decreased using the free gates(since the content in the bracket is always greater than or equal to $$\pmb u_{t-1}$$ given that $$\pmb w_{t-1}^w\ge\pmb 0$$ --- it equals to $$\pmb u_{t-1}$$ only when $$\pmb w_{t-1}^w=\pmb 0$$. One could also take the part inside bracket as a set union operation, where $$\pmb u_{t-1}$$ and $$\pmb w_{t-1}^w$$ are two sets and $$\pmb u_{t-1}\circ\pmb w_{t-1}^w$$ is their intersection.

4. Once $$\pmb u_t$$ has been determined, the free list $$\pmb\phi_t\in\mathbb Z^N$$ is defined by sorting the indices of the memory locations in ascending order of usage.

5. The allocation weighting $$\pmb a_t\in\Delta_N$$, which is used to provide new locations for writing, is defined as
   
$$
   \pmb a_t[\pmb\phi_t[j]]=(1-\pmb u_t[\pmb\phi_t[j]])\prod_{i=1}^{j-1}\pmb u_t[\pmb\phi_t[i]]
   $$

   Intuitively, the allocation weighting $$\pmb a_t[\pmb\phi_t[j]]$$ is in direct proportion to usages of locations that have more available space than $$\pmb\phi_t[j]$$ --- the more usages of these locations are used, the more memory at $$\pmb \phi_t[j]$$ is allocated --- and in inverse proportion to usage of $$\pmb\phi_t[j]$$ itself. The former guarantees always filling locations with more available space.

As noted by the authors, the sort operation in step 4 induces discontinuities at the points at which the sort order changes. We ignore these discontinuities when calculating the gradient, as they do not seem to be relevant to learning.

Noticeably, the allocation mechanism is independent of the memory size(in fact, all elements in the interface vector are), meaning that DNCs can be trained to solve a task using one size of memory and later upgraded to a larger memory without retraining.

#### <a name='ww'></a>Write Weighting

Now we combine the content-based addressing and dynamic memory allocation to form the write weighting. We use the memory from the previous time step to compute the content weighting

$$
\pmb c_t^w=\mathcal C(\pmb M_{t-1},\pmb k_t^w,\beta_t^w)
$$

A write weighting can be written as

$$
\pmb w_t^w=g_t^w[g_t^a\pmb a_t+(1-g_t^a)\pmb c_t^w]
$$

where the allocation gate $$g_t^a\in[0,1]$$ governs the interpolation and the write gate $$g_t^w\in[0,1]$$ determines how much will be written to the memory. 

#### <a name='tml'></a>Temporal Memory Linkage

The mechanisms we discussed so far stores no information about the order in which the memory locations are written to. However, there are many situations in which retaining this information is useful: for example, when a sequence of instructions must be recorded and retrieved in order. Graves&Wayne et al. propose to keep track of consecutively modified memory locations, thereby enabling a DNC to recover sequences in the written order. This is done as follows

1. We first introduce a precedence weighting $$\pmb p_t\in\Delta_N$$, where element $$\pmb p_t[i]$$ represents the degree to which location $$i$$ was the last one written to. We define $$\pmb p_t$$ by the recurrence relation:
   
$$
   \begin{align}
   \pmb p_0&=\pmb 0\\\
   \pmb p_t&=\left(1-\sum_i\pmb w_t^w[i]\right)\pmb p_{t-1}+\pmb w_t^w
   \end{align}
   $$

   where $$\pmb w_t^w$$ is the write weighting. Notice that $$\pmb p_t$$ is updated based on the write weighting $$\pmb w_t^w$$ at the current time step: If $$\sum_i\pmb w_t^w[i]\approx 0$$, which means there is barely any writing happened at the current time step, then $$\pmb p_t\approx\pmb p_{t-1}$$, indicating that the write history is carried over. On the other hand, if $$\sum_i\pmb w_t^w[i]\approx 1$$, the previous precedence is nearly replaced, so $$\pmb p_t\approx \pmb w_t^w$$.

2. Now we define the temporal link matrix $$\pmb L_t$$, where $$\pmb L_t[i,j]$$ represents the degree to which location $$i$$ was written to after location $$j$$ was written to(one could regard it as a variant of an adjacent matrix). We can further induce that the rows of $$\pmb L_t$$, e.g., $$\pmb L_t[i,\cdot]$$, represents the degrees to which each location was written to before memory slot $$i$$. Similarly, the columns of $$\pmb L_t$$, e.g. $$\pmb L_t[\cdot, j]$$, represents the degrees to which different locations were written to after memory slot $$j$$. 
  
   Every time a location is modified, we want to update the link matrix to remove old links to and from that location and add new links from the last-written location. This is done by the following recurrence relation
   
$$
   \begin{align}\pmb L_0[i,j]&=0\qquad\forall i,j\\\
   \pmb L_t[i,i]&=0\qquad\forall i\\\
   \pmb L_t[i,j]&=(1-\pmb w_t^w[i]-\pmb w_t^w[j])\pmb L_{t-1}[i,j]+\pmb w_t^w[i]\pmb p_{t-1}[j]
   \end{align}
   $$

   We exclude self-links since it is unclear how to follow a transition from a location to itself. Noticeably, we use the precedence weighting from the previous time step in the above update, which is sensible because of the definition of the link matrix $$\pmb L_t$$. We can vectorize the above process as follows

   
$$
   \begin{align}
    \hat {\pmb L}_t&=(\pmb E-\pmb w_t^w\pmb 1^\top-\pmb 1^{\top}(\pmb w_t^w)^\top)\circ\pmb L_{t-1}+\pmb w_t^w(\pmb p_{t-1})^\top\\\
    \pmb L_t&=\hat{\pmb L_t}\circ(\pmb E-\pmb I)\qquad\text{removes self-links}
    \end{align}
   $$


   where $$\pmb E$$ is a matrix of ones and $$\pmb I$$ is an identity matrix. 

   Before we continue, let's take some time to rationalize $$(1-\pmb w_t^w[i]-\pmb w_t^w[j])$$. $$-\pmb w_t^w[i]$$ says that the more we write to memory slot $$i$$, the less we retain the corresponding link entry. Especially, when $$\pmb w_t^w[i]=1$$, we retain no more about $$\pmb L_{t-1}[i,j]$$ and our new $$\pmb L_t[i,j]$$ completely depends on $$\pmb w_t^w[i]\pmb p_{t-1}[j]$$. The rationality of $$\pmb w_t^w[j]$$ works in a similar way. The more memory slot $$j$$ is written to, the newer memory at location $$j$$ is and the less the corresponding link entry should be retained. When $$\pmb w_t^w[j]=1$$, i.e., memory at location $$j$$ is completely refreshed by the new write vector, all $$L_t[\cdot,j]$$ are reset. 

3. We now designate the forward weighting $$\pmb f_t^i\in\Delta_N$$ and backward weighting $$\pmb b_t^i\in\Delta_N$$for read head $$i$$ based on the temporal link matrix
    
$$
    \begin{align}
    \pmb f_t^i=\pmb L_t\pmb w_{t-1}^{r,i}\\\
    \pmb b_t^i=\pmb L_t^\top\pmb w_{t-1}^{r,i}
    \end{align}
    $$

   where $$\pmb w_{t-1}^{r,i}$$ is the $$i$$th read weighting from the previous time step. Intuitively, the forward weighting specifies what the head is going to read after the previous reading in the sequential written order, and the backward weighting specifies that in the inverse written order.

Also, Graves&Wayne et al. propose a trick to reduce the overhead of the link matrix. I omit it since I'm not so sure if it worths the effort for that we can vectorize the update.

#### Read Weighting

Each read head leverages a read mode vector $$\pmb\pi _t^i\in\mathcal S_3$$ to weight the backward weighting $$\pmb b_t^i$$, the forward weighting $$\pmb f_t^i$$ and the content read weighting $$\pmb c_t^{r,i}=\mathcal C(\pmb M_{t},\pmb k_t^{r,i},\beta_t^{r,i})$$ as follows

$$
\pmb w_t^{r,i}=\pmb\pi_t^i[1]\pmb b_t^i+\pmb\pi_t^i[2]\pmb c_t^{r,i}+\pmb\pi_t^i[3]\pmb f_t^i
$$


## Comparison with The Neural Turing Machine

The neural Turing machine (NTM) was the predecessor to the DNC. It used a similar architecture of neural network controller with read-write access to a memory matrix, but differed in the access mechanism used to interface with the memory. In the NTM, content-based addressing was combined with location-based addressing to allow the network to iterate through memory locations in order of their indices (for example, location $$n$$ followed by $$n + 1$$ and so on). This allowed the network to store and retrieve temporal sequences in contiguous blocks of memory. However, there were several drawbacks:

1. The NTM has no mechanism to ensure that blocks of allocated memory do not overlap and interfere—a basic problem of computer memory management. Interference is not an issue for the dynamic memory allocation used by DNCs, which provides single free locations at a time, irrespective of index, and therefore does not require contiguous blocks. 
2. The NTM has no way of freeing locations that have already been written to and, hence, no way of reusing memory when processing long sequences. This problem is addressed in DNCs by the free gates used for de-allocation. 
3. Sequential information is preserved only as long as the NTM continues to iterate through consecutive locations; as soon as the write head jumps to a different part of the memory (using content-based addressing) the order of writes before and after the jump cannot be recovered by the read head. The temporal link matrix used by DNCs does not suffer from this problem because it tracks the order in which writes were made.

## References

Alex Graves, Greg Wayne, Malcolm Reynolds, Tim Harley, Ivo Danihelka, Agnieszka Grabska-Barwińska, Sergio Gómez Colmenarejo, Edward Grefenstette, Tiago Ramalho, John Agapiou, Adrià Puigdomènech Badia, Karl Moritz Hermann, Yori Zwols, Georg Ostrovski, Adam Cain, Helen King, Christopher Summerfield, Phil Blunsom, Koray Kavukcuoglu & Demis Hassabis. 2016. “Hybrid Computing Using a Neural Network with Dynamic External Memory.” *Nature* 538 (7626): 471–76. https://doi.org/10.1038/nature20101.

Hsin, Carol. 2016. “Implementation and Optimization of Differentiable Neural Computers.” https://web.stanford.edu/class/cs224n/reports/2753780.pdf.

Code: https://github.com/deepmind/dnc