---
title: "DQN — Deep Q Network"
excerpt: "In which we talk about Deep Q network(DQN), a successful algorithm works in discrete-action environments"
categories:
  - Reinforcement Learning
tags:
  - Reinforcement Learning
---

## <a name="DQN"></a>Deep Q Network (DQN)

Value-based algorithms are notoriously unstable when working with deep neural networks due to the correlation of input data. In 2015, DeepMind introduced a deep Q network which employed experience replay, target network, and error clipping to stabilize algorithms.

### Algorithm Overview

![DQN]({{ '/images/rl/DQN.png' | absolute_url }})

### Q Network

#### Transition/Experience

Transitions/Experiences are tuples used to train the network: $$ <\phi_t, a_t, r_t, \phi_{t+1}> $$, where $$ \phi_t $$ is the result of preprocessing state $$ s_t $$.

#### Input

$$ \phi_t $$. 

In the context of *Atari 2600*, we

1. to encode a single frame, take the maximum value for each pixel color value over the frame being encoded and the previous frame so as to remove flickering. 
2. then extract the *Y channel*, AKA luminance, from the RGB frame and rescale it to $$ 84\times84 $$. 

Apply the above preprocessing to the $$ m $$ most recent frames and stacks them to produce the input, in which $$ m=4 $$. Frame stacking can provide us some contextual information, such as object speed and moving direction.

#### Output

$$ Q(\phi_t, a) $$ for *each* valid action $$ a $$

#### Loss


$$
L(\theta)=\mathbb E\left[\left(r+\gamma \max_{a'}\hat Q(s', a';\theta^-)-Q(s, a;\theta)\right)^2\right]
$$


Where $$ r+\gamma\max_{a'}\hat Q(s', a';\theta^-) $$ is the approximate target value, $$ Q(s, a;\theta) $$ is the predicted value. $$ \mathbb E $$ results in the *emperical mean loss* over a minibatch of samples.

### Auxiliary Technique to Stabilize Network

Traditional reinforcement learning is unstable or even diverges when cooperating with nonlinear functions such as neural networks because of the nature of the correlation between the samples. DQN introduces the following techniques to alleviate such issues and thereby stabilizes the algorithm 

#### Experience Replay

In which we store the agent's transition(experience) at each time-step $$ e_t=(s_t, a_t, r_t, s_{t+1}) $$, in a data set $$ D=\{e_1, …, e_t\} $$. Note that $$ D $$ contains transitions over many episodes. During the inner loop of the algorithm, we randomly sample a small minibatch (e.g. of size $$ 32 $$) of transitions from $$ D $$ to update $$ Q $$ network.

##### Effects

This approach has several advantages over standard online Q-learning

1. Each step of transition is potentially used in many weight updates, which allows for greater data efficiency
2. Learning directly from consecutive samples is inefficient, owing to the strong correlations between the samples. Randomizing the samples breaks these correlations and therefore reduces the variance of the updates

##### Notes

When learning by experience replay, it is necessary to learn *off-policy* (therefore, importance sampling should be considered), because our current parameters are different from those used to generate the samples

##### Further Potiential Improvements

The data structure of $$ D $$ is generally a FIFO queue. We may gain some improvements by making it a priority queue (weighing transitions based on their impact on learning, in other words, prioritizing the ones from which we can learn most), and drop those less valuable when $$ D $$ is full.

#### Target Network $$ \hat Q $$

We use a separate target network $$ \hat Q $$, which is periodically updated to $$ Q $$, to generate the targets $$ y_i $$ in the Q-learning update. Note that, in the algorithm, the target network takes an action *different* from that taken by $$ Q $$. This makes the target network more stable to the change of the update. 

##### Effects

It makes the algorithm more stable compared to standard online Q-learning, where an update that increases $$ Q(s_t, a_t) $$ often also increase $$ Q(s_{t+1}, a) $$ for all $$ a $$ and hence also increases the target $$ y_i $$, possibly leading to oscillations or divergence of the policy. Furthermore, a target network can ease the problem of catastrophic forgetting; see this answer for details: https://stackoverflow.com/a/54238556

#### Error Clipping

Which defines the *loss function* as below

$$
\begin{align}
L(y, \hat y )=\begin{cases}
{1\over 2}(y-\hat y)^2, &|y-\hat y|\le1\\\ 
\left|y-\hat y\right|,&otherwise
\end{cases}
\end{align}
$$

Where $$ y $$ is the target value and $$ \hat y $$ is the predicted value

This changes the gradients to

$$
\begin{align}
\nabla_{\hat y} L(y, \hat y )=\begin{cases}
-(y-\hat y)\nabla\hat y, &|y-\hat y|\le1\\\ 
-\nabla \hat y,&y-\hat y > 1\\\ 
\nabla \hat y, &y-\hat y < -1
\end{cases}
\end{align}
$$


##### Effects

It further improves the stability of the algorithm

##### Extension — Huber loss


$$
\begin{align}
L(y, \hat y )=\begin{cases}
{1\over 2}(y-\hat y)^2, &|y-\hat y|\le\delta\\\ 
\delta \left|y-\hat y\right|-{1\over2}\delta^2,&otherwise
\end{cases}
\end{align}
$$


### Reward Clipping

While we evaluated our agents on unmodified games, we made one change to the reward structure of the games during training only. As the scale of scores varies greatly from game to game, we clipped all positive rewards at $$1$$ and all negative rewards at $$-1$$, leaving $$0$$ rewards unchanged. Clipping the rewards in this manner limits the scale of the error derivatives and makes it easier to use the same learning rate across multiple games. At the same time, it could affect the performance of our agent since it cannot differentiate between rewards of different magnitude. For games where there is a life counter, the Atari 2600 emulator also sends the number of lives left in the game, which is then used to mark the end of an episode during training.