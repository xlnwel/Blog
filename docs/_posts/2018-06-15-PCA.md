---
title: "PCA and Whitening"
excerpt: "In which we talk about dimensionality reduction technique PCA, and its derivatives whitening and ZCA whitening"
categories:
  - Machine Learning
tags:
  - Machine Learning
  - Mathematics
---

## Introduction

PCA is a dimensionality-reduction technique wildly used in data preprocessing. This post will walk through PCA and its derivatives---whitening and ZCA whitening.

**Note**: dimensionality reduction techniques highly rely on the assumption that features contains significant redundancy, which might not always be true in practice (features are usually designed with their unique contributions and removing any of them may affect the training accuracy to some degree.[Guolin Ke et al. 2017])

### <a name="dir"></a>Table of Contents

- [Singular Value Decomposition](#SVD)
- [Principal Component Analysis](#PCA)
- [Whitening](#white)
- [ZCA whitening](#ZCA)

### <a name="SVD"></a>Singular Value Decomposition

Any real matrix $$ A $$ can be decomposed into

$$
A= U\Sigma^{1\over 2} V^T
$$

where 

- $$ U $$ is the *left-singular eigenvectors*, a matrix composed of *orthonormal eigenvectors* of $$ AA^T $$, 
- $$ \Sigma^{1\over 2}$$ is a diagonal matrix whose diagonal entries are the *singular values*, i.e., square roots of the eigenvalues of both $$ AA^T $$ and $$ A^TA $$
- $$ V $$ is the *right-singular eigenvectors*, a matrix composed of *orthonormal eigenvectors* of $$ A^TA $$ .

Therefore, we also have


$$
A^TA=V\Sigma V^T
$$


In addition, if $$ A $$ is a *real symmetric matrix* (which results in $$ AA^T=A^TA $$), $$ U $$ will be identical to $$ V $$ and $$ \Sigma^{1\over 2} $$ will be a diagonal matrix whose diagonal entries are just the eigenvalues of $$ A $$. In this way, $$ A $$ could be rewritten as


$$
A=V\Sigma^{1\over 2} V^T
$$


[Elevator back to directory](#dir)

### <a name="DR_SVD"></a>Dimensionality Reduction with SVD

In order to use SVD to reduce dimensionality, we take the matrix multipliciation of truncated $$U$$ and $$\Sigma^{1\over 2}$$:

$$
\tilde A=U_{:, 1:k}\Sigma_{1:k, 1:k}^{1\over 2}
$$


### <a name="PCA"></a>Principal Component Analysis

PCA is a technique to break the linear correlation of variables so that the covariance matrix of the resultant data is diagonal. In that case, we can use the most variant features to simplify the input data without loss of much information.

#### Algorithm

In order to perform PCA, 

- First do mean and variance normalization on the input $$ X $$: subtract the mean and divide each dimension of the centered data by the corresponding standard deviation. subtracting the mean helps avoid potential numerical issues; dividing by the corresponding standard deviation is important when the variables are note measured on the same scale
- Then, to uncorrelate the input $$ X $$, rotate $$ X $$ using the orthogonal rotation matrix $$ V $$, the *orthonormal eigenvectors* of $$ X^TX $$ (i.e., the *right-singular eigenvectors* of $$ X $$)


$$
X_{PCA}=XV
$$


#### Proof of Uncorrelation

we could verify that the resultant data $$ X_{PCA} $$ is uncorrelated by proving the covariance matrix $$ C_{PCA}=X^T_{PCA}X_{PCA} $$ is a diagonal matrix:

$$
\begin{align}
X_{PCA}^TX_{PCA}&=V^TX^TXV \\\ 
&=V^TV\Sigma V^TV\\\ 
&=\Sigma
\end{align}
$$


where $$ \Sigma $$ is a diagonal matrix whose diagonal entries are the eigenvalues of $$ X^TX $$

#### Dimension Reduction

Usually we only retrain the top $$ k $$ components by taking only first $$ k $$ columns (associated to the largest $$ k $$ singular values of $$X$$) in $$ V $$, where $$ k $$ is the smallest value that satisfies

$$
{\sum_{j=1}^k \lambda_j\over\sum_{j=1}^n\lambda_j}\ge0.99
$$

where $$ \lambda_i $$ is the $$ i $$-th eigenvalue. 

The above inequality says to retain $$ 99\% $$ of variance. In practice, this could be reduced to $$ 90\%+ $$ depending on a specific application

[Elevator back to directory](#dir)

### <a name="white"></a>Whitening

Whitening further standardizes the result of PCA so that they have variance $$ 1 $$

$$
X_{whiten}=X_{PCA}\Sigma^{-1/2}
$$


After whitening, the data has covariance equal to the identity matrix $$ I $$ since

$$
\begin{align}
X_{whiten}^TX_{whiten}&=\Sigma^{-1/2}X_{PCA}X_{PCA}\Sigma^{-1/2}\\\ 
&=\Sigma^{-1/2}\Sigma\Sigma^{-1/2}\\\ 
&=I
\end{align}
$$


#### Regularization

In practice, sometimes some of eigenvalues $$ \lambda_i $$ in $$ \Sigma $$ will be close to $$ 0 $$, and thus whitening step, where each column $$ X_{:,i} $$ in $$ X_{PCA} $$ is divided by $$ \sqrt{\lambda_i} $$, would cause the data to blow up. To maintain numerically stablility, we usually use a small amount of regularization, and add a small constant $$ \epsilon $$ to the eigenvalues before taking their square roots and inverse

$$
X_{whiten}=X_{PCA}(\Sigma+\mathrm {diag}(\epsilon))^{-1/2}
$$

when $$ X $$ takes value around $$ [-1, 1] $$, a value of $$ \epsilon\approx 10^{-5} $$ might be typical.

For the case of images, adding $$ \epsilon $$ also has the effect of slightly smoothing the input image. This also has a desirable effect of removing aliasing artifacts caused by the way pixels are laid out in an image and can improve the features learned

[Elevator back to directory](#dir)

### <a name="ZCA"></a>ZCA Whitening

Since whitening standardizes all components so that they have covariance $$ I $$, any rotation applied to the whitened data should stay whitened. That is, for any orthogonal matrix $$ R $$, $$ X_{whiten}R^T $$ is also whitened. In ZCA whitening, we choose $$ R=V $$. Thus, we have

$$
X_{ZCAwhiten}=X_{whiten}V^T
$$

The resultant data is as close as possible to the original data (in the least squares sense). That is, ZCA whitening minimizes $$ \Vert X-XA^T\Vert_2^2 $$ subject to $$ XA^T $$ being whitened, where $$ A $$ is a transform matrix â€” in case of ZCA whitening, $$ A=V(\Sigma+\mathrm diag(\epsilon))^{-1/2} V^T $$

[Elevator back to directory](#dir)